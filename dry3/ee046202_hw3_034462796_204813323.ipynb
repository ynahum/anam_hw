{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oipggHE7gZRl"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Homework 3 - VAE, GAN & Clustering\n",
    "---\n",
    "### <a style='color:red'> Due Date: 26.01.2021 </a>\n",
    "\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Questions\n",
    "    * VAE Theory\n",
    "    * Fun with GANs\n",
    "    * Expectation Maximization\n",
    "\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvGB_yqhgZRp"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Yair Nahum| nahum.yair@campus.technion.ac.il| 034462796|\n",
    "|David Regev| regev.david@campus.technion.ac.il| 204813323|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQeWCTP4gZRq"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
    "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
    "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **ANSWERS TO THEORETICAL/MATHEMATICAL QUESTIONS**:\n",
    "    * **Typed - 5 points bonus**: you can type directly in a Markdown cell using Latex (see cheatsheets above), or use Word, Overleaf, LyX...\n",
    "        * This is a really good practice, we encourage you to practice your math typing skills.\n",
    "    * **Handwritten** - if we can't read your handwriting, we will automatically take off the points of the questions. Please write clearly. No bonus for handwritten submissions.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_hw3_id1_id2.ipynb`.\n",
    "    * If you answered the questions in a different file you should submit a `.pdf` file with the name `ee046202_hw3_id1_id2.pdf`.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB6XIz3NgZRs"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1qSa6qygZRt"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - VAE Theory\n",
    "---\n",
    "As you recall, the objective function of the VAE is: $$ \\mathcal{L}_{VAE} = -\\mathbb{E}_{Q(z|X)}[\\log P(X|z)] + D_{KL}[Q(z|X)|| P(z)] $$ The first term is called the \"reconstruction error\" and the second is the KL-divergence. In this question we are going to derive mathematic properties of these terms.\n",
    "\n",
    "\n",
    "1.In the tutorial, you have seen how the reconstruction error for the Gaussian case is the MSE. For images, with pixel values in [0,1], this loss function is not that great, and it is better to use the **Binary Cross Entropy (BCE)** loss, which is defined as: $$ BCE = -[y \\log(p) +(1-y)\\log(1-p)] $$\n",
    "* $y$ - the true pixel value of the original image, $y \\in \\{0,1\\}$\n",
    "* $p$ - the output from the neural netwrok (estimated probabiliy that the pixel is 1), $p \\in [0,1]$\n",
    "   Assume that $P(X|z)$ is Benoulli-distributed, that is, $P(X|z) \\sim Bern(\\phi(x,z))$: $$ P(X|z) = \\begin{cases} \\phi(x,z) && \\text{ if } y=1 \\\\ 1 -\\phi(x,z) && \\text{ if } y=0 \\end{cases} $$\n",
    "    * $\\phi(x,z)$ is the output of the decoder for each pixel in the input image.\n",
    "    \n",
    "Show that $$-\\log P(x|z) = BCE.$$\n",
    "\n",
    "2.As you recall, we usually model the prior as $P(Z) \\sim \\mathcal{N}(0,I)$, but sometimes we want to model the prior as some other, perhaps more expressive distribution. We can even model it as a neural network, that is, $P(Z|c) \\sim \\mathcal{N}(\\mu_c, \\Sigma_c)$ where $\\mu_c, \\Sigma_c$ are the outputs of a neural network and $c$ is the input of the network. Derive the closed-form solution of the KL-divergence for this case. That is, derive: $$ D_{KL}[q_{\\phi}(z|X)|| p(z|c)] $$ You should express your answer with $\\mu_q, \\Sigma_q, \\mu_c, \\Sigma_C$ (no $z$'s). Recall that $q_{\\phi}(z|X) \\sim \\mathcal{N}(\\mu_q\\, \\Sigma_q)$. Use the same assumptions from the tutorial. Don't forget that the expectancy is over $z|x \\sim q(z|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HLAGMXxgZRw"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Answers - Q1\n",
    "### Q1 - section 1\n",
    "\n",
    "Another way to write the PDF of Bernoulli is:<br>\n",
    "$$P(x|z)=\\phi(x,z)^y\\cdot(1-\\phi(x,z))^{(1-y)} \\implies$$<br>\n",
    "$$log(P(x|z))=log(\\phi(x,z)^y\\cdot(1-\\phi(x,z))^{(1-y)}) = log(\\phi(x,z)^y)+log((1-\\phi(x,z))^{(1-y)}) =  ylog(\\phi(x,z))+(1-y)log(1-\\phi(x,z)) \\implies$$<br>\n",
    "$$-log(P(x|z))=-[ylog(p)+(1-y)log(1-p)]=BCE$$\n",
    "\n",
    "### Q1 - section 2\n",
    "\n",
    "$$D_{KL}[q_\\phi(z|x)||p(z|c)]=D_{KL}[\\mathcal{N}(\\mu_q\\, \\Sigma_q)||\\mathcal{N}(\\mu_c\\, \\Sigma_c)]=\\mathbb{E}_{q(z|X)}[log\\frac{(2\\pi)^-\\frac{d}{2}\\cdot|\\Sigma_q|^-{\\frac{1}{2}}\\cdot e^-\\frac{1}{2}(z-\\mu_q)^T)\\Sigma_q^{-1}(z-\\mu_q)}{(2\\pi)^-\\frac{d}{2}\\cdot|\\Sigma_c|^-{\\frac{1}{2}}\\cdot e^-\\frac{1}{2}(z-\\mu_c)^T)\\Sigma_c^{-1}(z-\\mu_c)}]=\\mathbb{E}_{q(z|X)}[-\\frac{1}{2}log|\\Sigma_q|-\\frac{1}{2}(z-\\mu_q)^T\\Sigma_q^{-1}(z-\\mu_q)+\\frac{1}{2}log|\\Sigma_c|+\\frac{1}{2}(z-\\mu_c)^T\\Sigma_c^{-1}(z-\\mu_c)]$$<br>\n",
    "$$=-\\frac{1}{2}(log|\\Sigma_q|-log|\\Sigma_c|)-\\frac{1}{2}\\mathbb{E}_{q_{\\phi}(z|X)}[(z-\\mu_q)^T)\\Sigma_q^{-1}(z-\\mu_q)-(z-\\mu_c)^T\\Sigma_c^{-1}(z-\\mu_c)]$$<br>\n",
    "$$=-\\frac{1}{2}(log|\\Sigma_q|-log|\\Sigma_c|)-\\frac{1}{2}\\mathbb{E}_{q_{\\phi}(z|X)}[z^T\\Sigma_q^{-1}z-2z^T\\Sigma_q^{-1}\\mu_q+\\mu_q^T\\Sigma_q^{-1}\\mu_q-z^T\\Sigma_c^{-1}z+2z^T\\Sigma_c^{-1}\\mu_c-\\mu_c^T\\Sigma_c^{-1}\\mu_c]$$<br>\n",
    "$$=-\\frac{1}{2}log(\\frac{|\\Sigma_q|}{|\\Sigma_c|})-\\frac{1}{2}(\\mu_q^T\\Sigma_q^{-1}\\mu_q-\\mu_c^T\\Sigma_c^{-1}\\mu_c)+(\\mathbb{E}_{q_{\\phi}(z|x)}[z]^T)\\Sigma_q^{-1}\\mu_q-(\\mathbb{E}_{q_{\\phi}(z|x)}[z]^T)\\Sigma_c^{-1}\\mu_c-\\frac{1}{2}\\mathbb{E}_{q_{\\phi}(z|X)}[z^T\\Sigma_q^{-1}z-z^T\\Sigma_c^{-1}z]$$<br>\n",
    "We require a diagonal covariance matrices. therefore we can deduce:\n",
    "$$=-\\frac{1}{2}\\sum_{i=1}^{d}log(\\frac{\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}})+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mu_q(X)_i^2}{\\Sigma_q(X)_{ii}}-\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mu_c(X)_i^2}{\\Sigma_c(X)_{ii}}-\\frac{1}{2}\\mathbb{E}_{q_{\\phi}(z|x)}[\\sum_{i=1}^{d}\\frac{z_i^2}{\\Sigma_q(X)_{ii}}-\\sum_{i=1}^{d}\\frac{z_i^2}{\\Sigma_c(X)_{ii}}]$$<br>\n",
    "Recall from the recitation that: \n",
    "$$\\mathbb{E}[z_i^2]=\\Sigma(X)_{ii}+\\mu(X)_i^2$$\n",
    "Therefore:\n",
    "$$=-\\frac{1}{2}\\sum_{i=1}^{d}log(\\frac{\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}})+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mu_q(X)_i^2}{\\Sigma_q(X)_{ii}}+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mu_c(X)_i^2}{\\Sigma_c(X)_{ii}}-\\sum_{i=1}^{d}\\frac{\\mu_q(X)_i\\cdot\\mu_c(X)_i}{\\Sigma_c(X)_{ii}}-\\frac{1}{2}\\sum_{i=1}^{d}[1+\\frac{\\mu_q(X)_i^2}{\\Sigma_q(X)_{ii}}-\\frac{\\mu_q(X)_i^2+\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}}]$$<br>\n",
    "$$=-\\frac{1}{2}\\sum_{i=1}^{d}log(\\frac{\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}})+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mu_c(X)_i^2}{\\Sigma_c(X)_{ii}}+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mu_q(X)_i^2+\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}}-\\sum_{i=1}^{d}\\frac{\\mu_q(X)_i\\cdot\\mu_c(X)_i}{\\Sigma_c(X)_{ii}}-\\frac{d}{2}$$<br>\n",
    "$$=\\frac{1}{2}\\sum_{i=1}^{d}[\\frac{\\mu_c(X)_i^2+\\mu_q(X)_i^2+\\Sigma_q(X)_{ii}-2\\cdot\\mu_q(X)_i\\cdot\\mu_c(X)_i}{\\Sigma_c(X)_{ii}}-1-log(\\frac{\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}})] =$$<br>\n",
    "$$\\frac{1}{2}\\sum_{i=1}^{d}[\\frac{(\\mu_q(X)_i-\\mu_c(X)_i)^2+\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}}-1-log(\\frac{\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}})]$$<br>\n",
    "\n",
    "We would like to examine if the result aligns with the result from the recitation:<br>\n",
    "For $\\mu_c=0$ and $\\Sigma_c=I$, we get:\n",
    "$$D_{KL}[\\mathcal{N}(\\mu_q\\, \\Sigma_q)||\\mathcal{N}(\\mu_c\\, \\Sigma_c)]=D_{KL}[\\mathcal{N}(\\mu_q\\, \\Sigma_q)||\\mathcal{N}(0, I)]$$<br>\n",
    "$$=\\frac{1}{2}\\sum_{i=1}^{d}[\\frac{(\\mu_q(X)_i-\\mu_c(X)_i)^2+\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}}-1-log(\\frac{\\Sigma_q(X)_{ii}}{\\Sigma_c(X)_{ii}})]$$<br>\n",
    "$$=\\frac{1}{2}\\sum_{i=1}^{d}[\\frac{(\\mu_q(X)_i-0)^2+\\Sigma_q(X)_{ii}}{1}-1-log(\\frac{\\Sigma_q(X)_{ii}}{1})]$$<br>\n",
    "$$=\\frac{1}{2}\\sum_{i=1}^{d}[\\Sigma_q(X)_{ii}+\\mu_q(X)_i^2-1-log\\Sigma_q(X)_{ii}]$$<br>\n",
    "as we expected from the recitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_12ZmyABgZRz"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Generative Adversarial Networks (GANs)\n",
    "---\n",
    "1. Recall that when the discriminator gets *too good*, generator gradient vanishes and learns nothing - **Vanisihng/Diminishing Gradient**. Consider the second term in the objective function which is relavent only for the generator: $$ \\mathbb{E}_{z\\sim q(z)} \\left[\\log(1 - D\\left(G(z)\\right)) \\right].$$ Show that if $D$ is confident (that the sample is fake - its output is 0), the gradient of the generator goes to 0.\n",
    "    * Hint: recall that the output of binary classification is the output of the *sigmoid* function, $\\sigma$.\n",
    "    \n",
    "2. In the proof of Nash equilibrium, we used the Jensen-Shannon Divergence (JSD): $$ JSD( p_G\\mid \\mid p_{data}) = JSD(p_{data} \\mid \\mid p_G) = \\frac{1}{2}KL\\left(p_{data} \\mid \\mid \\left(\\frac{p_{data} + p_G}{2}\\right)\\right) +  \\frac{1}{2}KL\\left(p_G \\mid \\mid \\left(\\frac{p_{data} + p_G}{2}\\right)\\right).$$ Denote $p_{data}=a, p_G=b$. Show that $$ 0 \\leq JSD(a\\mid \\mid b) \\leq 1.$$\n",
    "    * For the KL, use $\\log_2.$\n",
    "    \n",
    "3. Let's say you trained a GAN to generate images of apples (and assume the GAN converged and you can now generate nice images of apples). \n",
    "    * Can the **Discriminator** be used to classify between apples and non-apples? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrpkDbBYsimk"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Answers - Q2\n",
    "### Q2 - section 1\n",
    "We will calculate the weight's gradient of $\\mathbb{E}_{z\\sim q(z)}[log(1-D(G(z)))]$<br>\n",
    "$$\\nabla_{W_G}\\mathbb{E}_{z\\sim q(z)}[log(1-D(G(z)))]=\\mathbb{E}_{z\\sim q(z)}[-\\frac{1}{1-D(G(z))}\\cdot \\nabla_{W_G}D(G(z))]$$\n",
    "The output of binary classification is the output of the isgmoid function $\\sigma$ so we can deriviate $D(G(z))$ as a sigmoid function:\n",
    "$$\\nabla_{W_G}D(G(z))=D(G(z))(1-D(G(z)))$$\n",
    "connect it together:\n",
    "$$\\nabla_{W_G}D(G(z))=\\mathbb{E}_{z\\sim q(z)}[-\\frac{1}{1-D(G(z))}\\cdot D(G(z))(1-D(G(z)))=-\\mathbb{E}_{z\\sim q(z)}[D(G(z))]=0$$\n",
    "\n",
    "the last transition is due to that *__D__* is confident and $D(G(z))=0$ and then $-\\mathbb{E}_{z\\sim q(z)}[0]=0$\n",
    "\n",
    "### Q2 - section 2\n",
    "\n",
    "Left side:<br>\n",
    "By jensen inequality for concave function we prove that $D_{KL}(P||Q)\\geq 0$<br>\n",
    "$$-D_{KL}(P||Q)=-\\sum_{x\\in X} P(x)log(\\frac{P(x)}{Q(x)}) = \\sum_{x\\in X} P(x)log(\\frac{Q(x)}{P(x)}) = \\mathbb{E}[log(\\frac{Q(x)}{P(x)})]\\leq log(\\mathbb{E}[\\frac{Q(x)}{P(x)}] = $$\n",
    "$$log(\\sum_{x\\in X} P(x)(\\frac{Q(x)}{P(x)})) = log(\\sum_{x\\in X} Q(x)) = log(1) = 0$$\n",
    "$$\\implies JSD(a||b)=\\frac{1}{2}KL(a||\\frac{a+b}{2})+\\frac{1}{2}KL(b||\\frac{a+b}{2})\\geq0+0=0$$\n",
    "\n",
    "Right side:<br>\n",
    "\n",
    "$$JSD(a||b)=\\frac{1}{2}KL(a||\\frac{a+b}{2})+\\frac{1}{2}KL(b||\\frac{a+b}{2})$$<br>\n",
    "$$\\frac{1}{2}\\sum_{a,b}a\\cdot log(\\frac{a}{\\frac{a+b}{2}})+\\frac{1}{2}\\sum_{a,b}b\\cdot log(\\frac{b}{\\frac{a+b}{2}})=\\frac{1}{2}\\sum_{a,b}a\\cdot log(2\\frac{a}{a+b})+\\frac{1}{2}\\sum_{a,b}b\\cdot log(2\\frac{b}{a+b})$$<br>\n",
    "$$=\\frac{1}{2}\\sum_{a,b}a\\cdot (log(2)+log(\\frac{a}{a+b}))+\\frac{1}{2}\\sum_{a,b}b\\cdot (log(2)+log(\\frac{b}{a+b}))=\\frac{1}{2}\\sum_{a,b}a\\cdot (1+log(\\frac{a}{a+b}))+\\frac{1}{2}\\sum_{a,b}b\\cdot (1+log(\\frac{b}{a+b}))$$<br>\n",
    "$$=\\frac{1}{2}\\sum_{a,b}a+a\\cdot log(\\frac{a}{a+b})+\\frac{1}{2}\\sum_{a,b}b+b\\cdot log(\\frac{b}{a+b})$$<br>\n",
    "As a and b are probabilities, their sum is 1<br>\n",
    "$$=\\frac{1}{2}(1+\\sum_{a,b}a\\cdot log(\\frac{a}{a+b}))+\\frac{1}{2}(1+\\sum_{a,b}b\\cdot log(\\frac{b}{a+b}))=1+\\frac{1}{2}\\sum_{a,b}a\\cdot log(\\frac{a}{a+b}))+\\sum_{a,b}b\\cdot log(\\frac{b}{a+b}))$$<br>\n",
    "$$=1+\\frac{1}{2}\\sum_{a,b}a\\cdot log(a)-a\\cdot log(a+b)+b\\cdot log(b)-b\\cdot log(a+b)$$<br>\n",
    "Log is an increasing monotonic function, therefore:\n",
    "$$a\\cdot log(a)\\leq a\\cdot log(a+b)$$ and $$b\\cdot log(b)\\leq b\\cdot log(a+b)$$<br>\n",
    "This implies:<br>\n",
    "$$=1+\\frac{1}{2}\\sum_{a,b}a\\cdot log(a)-a\\cdot log(a+b)+b\\cdot log(b)-b\\cdot log(a+b)\\leq1+\\frac{1}{2}\\sum_{a,b}a\\cdot log(a+b)-a\\cdot log(a+b)+b\\cdot log(a+b)-b\\cdot log(a+b)=1+0=1$$\n",
    "\n",
    "### Q2 - section 3\n",
    "\n",
    "No. At start we train the D to certain level (As not to have a too good D since then the G cannot train as we saw on section 1).<br>\n",
    "During the process of training both G and D. both are playing one against the other. <br>\n",
    "GAN is trained as a zero-sum game and when it is converging such that the Generator won, the discriminator lost the game and got confused and trained badly. it actually flips a coin and guesses which picture is real and which is fake.<br>\n",
    "As opposed to a binary real-fake apples classifier the weihgts the discriminator learned are meaningless and cannot be used to detect real from fake apple images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5nSOUTEgZR1"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Expectation Maximization\n",
    "---\n",
    "1. Prove Jensen's inequality for *concave* functions, which we used in the derivation of the EM algorithm. Show that for a concave function, $f(X)$, $$ \\mathbb{E}[f(X] \\leq f(\\mathbb{E}[X]) .$$\n",
    "2. Consider the EM algorithm as presented in the tutorial. Prove that if we replace $\\theta^{t+1}$ as defined in line 4, with $$ \\theta^{t+1} \\leftarrow \\overline{\\theta} ,$$ where $\\overline{\\theta}$ satisfies that, $$ \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\overline{\\theta}) \\right] \\geq \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\theta^t) \\right], $$ the modified EM algorithm is still assured to improve or to halt in each iteration.\n",
    "3. Show the following holds for any $\\{q_i\\}_{i=1}^n,$ $$ \\mathcal{F}(\\theta, \\{q_i(\\cdot)\\}_{i=1}^n) \\triangleq \\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(x_i, z)}{q_i(z)}\\right]$$ <br> $$ = \\sum_{i=1}^n-D_{KL}(q_i(z) || p(z|x_i, \\theta)) +\\log p(x_i|\\theta) $$\n",
    "4. Find $$\\{\\pi_l, \\mu_l, \\Sigma_l \\}_{l=1}^k = \\arg\\max_{\\theta}\\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i, z|\\theta) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOIoIDDq46Ms"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Answers - Q3\n",
    "### Q3 - section 1\n",
    "\n",
    "A function is concave if $\\forall x_1,x_0,\\alpha\\in [0,1], f((1-\\alpha)x_1+\\alpha x_0)\\geq (1-\\alpha)f(x_1) + \\alpha f(x_0)$.<br>\n",
    "For the discrete case we can generalize by induction this claim for $x_1,..., x_n, \\alpha_1,..., \\alpha_n \\text{ s.t } \\sum_{i=1}^n \\alpha_i=1$.<br>\n",
    "\n",
    "$$f(\\mathbb{E}[X])=f(\\sum_{i=1}^n x_i p(x_i))\\geq \\sum_{i=1}^n f(x_i) p(x_i)=\\mathbb{E}[f(X)]$$\n",
    "Where the transition is by the generalization of the claim and due to the fact that $\\sum_{i=1}^n p(x_i) =1$ as p is probability function.\n",
    "\n",
    "For the consecutive case, if we assume $f$ is also differentiable, then:\n",
    "$$f(x) \\leq f(x_0) + f'(x_0)[x-x_0] \\forall x,x_0$$\n",
    "If we define $x_0 = \\mathbb{E}[x]$ and put it in the above equation:\n",
    "$$f(x) \\leq f(\\mathbb{E}[x]) + m[x-\\mathbb{E}[x]]$$\n",
    "If we multiply by $P(x)$ which is none negative we get:\n",
    "$$f(x)P(x) \\leq f(\\mathbb{E}[x])P(x) + m(xP(x)-\\mathbb{E}[x]P(x))$$\n",
    "If we assume f is also integrable on $(-\\infty, \\infty)$ with P(x) as pdf function:\n",
    "$$\\int_{-\\infty}^{\\infty}f(x)P(x)dx \\leq \\int_{-\\infty}^{\\infty}f(\\mathbb{E}[x])P(x)dx + \\int_{-\\infty}^{\\infty}m(xP(x)-\\mathbb{E}[x]P(x))dx \\implies$$\n",
    "$$\\mathbb{E}[f(x)] \\leq f(\\mathbb{E}[x])\\int_{-\\infty}^{\\infty}P(x)dx + m(\\int_{-\\infty}^{\\infty}xP(x)dx-\\mathbb{E}[x]\\int_{-\\infty}^{\\infty}P(x)dx) = f(\\mathbb{E}[x])\\cdot1 + m(\\mathbb{E}[x]-\\mathbb{E}[x]) = f(\\mathbb{E}[x])$$\n",
    "\n",
    "\n",
    "### Q3 - section 2\n",
    "\n",
    "We saw in the tutorial that: $$\\mathcal{L}(X|\\theta)\\geq F(\\theta,\\{q_i(\\cdot)\\}_{i=1}^n)$$\n",
    "and for $q_i^{*}=p(z|x_i;\\theta)$:\n",
    "$$\\mathcal{L}(X|\\theta)=F(\\theta,\\{q_i^{*}(\\cdot)\\}_{i=1}^n)$$\n",
    "Given  $ \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\overline{\\theta}) \\right] \\geq \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\theta^t) \\right] $:\n",
    "$$ \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\overline{\\theta}) \\right] + H(q_i^{*}) \\geq \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\theta^t) \\right] + H(q_i^{*}) \\Rightarrow F(\\overline{\\theta}, \\{q_i^{*}(\\cdot)\\}_{i=1}^n)\\geq F(\\theta, \\{q_i^{*}(\\cdot)\\}_{i=1}^n) $$\n",
    "$$\\Leftrightarrow\\mathcal{L}(X|\\overline{\\theta})=F(\\overline{\\theta}, \\{q_i^{*}(\\cdot)\\}_{i=1}^n)\\geq F(\\theta, \\{q_i^{*}(\\cdot)\\}_{i=1}^n)=\\mathcal{L}(X|\\theta)$$\n",
    "\n",
    "as we saw in the tutorial:\n",
    "$$F(\\overline{\\theta}^{t}, \\{q_i^{*}(\\cdot)\\}_{i=1}^n)\\geq F(\\overline{\\theta}^{t-1}, \\{q_i^{*}(\\cdot)\\}_{i=1}^n)$$\n",
    "and:\n",
    "$$\\mathcal{L}(\\overline{\\theta}^{t-1})=F(\\overline{\\theta}^{t-1}, \\{q_i^{*}(\\cdot)\\}_{i=1}^n)\\leq F(\\overline{\\theta}^{t}, \\{q_i^{*})(\\cdot)\\}_{i=1}^n)=\\mathcal{L}(X|\\overline{\\theta}^{t})=F(\\overline{\\theta}^{t})$$\n",
    "\n",
    "Conclusion: by each iteration the log likelihood gets equal or bigger than the previous iteration.\n",
    "\n",
    "### Q3 - section 3\n",
    "\n",
    "$$\\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(x_i, z;\\theta)}{q_i(z)}\\right]=\\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(z|x_i;\\theta)p(x_i;\\theta)}{q_i(z)}\\right]=\\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(z|x_i;\\theta)}{q_i(z)} + log(p(x_i;\\theta))\\right]=\\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(z|x_i;\\theta)}{q_i(z)}\\right] + log(p(x_i;\\theta))$$\n",
    "the last transition is due to that $x_i$ is not dependent with z.<br>\n",
    "by definition of $D_{KL}: D_{KL}(P||Q)=-\\sum_{x\\in X} P(x)log(\\frac{Q(x)}{P(x)})$ therefore:\n",
    "$$\\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(z|x_i;\\theta)}{q_i(z)}\\right] + log(p(x_i;\\theta))=-\\sum_{i=1}^n D_{KL}(q_i(z)||p(z|x_i;\\theta)) + log(p(x_i;\\theta))$$\n",
    "\n",
    "### Q3 - section 4\n",
    "\n",
    "Notice, by definition from the lecture:<br>\n",
    "$\\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i, z|\\theta) \\right]=\\sum_l \\gamma_{il}\\left[log(\\pi_l N(x_i|\\mu_l, \\Sigma_l))\\right]$<br>\n",
    "\n",
    "From the first tutorial, we saw matrix derviatives:\n",
    "* $\\nabla_x Ax = A^{T}$\n",
    "* $\\nabla_x x^{T} A x = (A + A^{T}) x$ \n",
    "* $\\frac{\\partial}{\\partial A} \\ln |A| = A^{-T}$\n",
    "* $\\frac{\\partial}{\\partial A} Tr[AB] = B^{T}$\n",
    "\n",
    "Using the above, we will use the following:\n",
    "1. $\\nabla_{\\mu} {\\mu}^{T} \\Sigma^{-1} x_i = \\Sigma^{-1} x_i$ \n",
    "2. $\\nabla_{\\mu} {\\mu}^{T} \\Sigma^{-1} \\mu = (\\Sigma^{-1} + {\\Sigma}^{-T}) \\mu$\n",
    "3. $\\frac{\\partial}{\\partial \\Sigma^{-1}} \\ln |\\Sigma^{-1}| = \\Sigma^{T} = \\Sigma$\n",
    "4. $\\frac{\\partial}{\\partial \\Sigma^{-1}} Tr[\\Sigma^{-1} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}] = \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}$\n",
    "\n",
    "We will derivate a specific instance i, and will change l to k for not confusing during the process.\n",
    "Also, note that $\\Sigma^{-1}$ is symmetric.\n",
    "\n",
    "$\\mu_k$:<br>\n",
    "$$\\nabla_{\\mu_k}\\sum_l \\gamma_{il}\\left[log(\\pi_l N(x_i|\\mu_l, \\Sigma_l))\\right]=\\nabla_{\\mu_k}\\sum_l \\gamma_{il}\\left[log(\\frac{1}{2\\pi|\\Sigma_l|})-\\frac{1}{2}(x_i-\\mu_l)^T\\Sigma^{-1}(x_i-\\mu_l)\\right]=\\nabla_{\\mu_k}\\sum_l \\gamma_{il}\\left[-\\frac{1}{2}(x_i-\\mu_l)^T\\Sigma^{-1}(x_i-\\mu_l)\\right]=-\\frac{1}{2}(\\gamma_{ik}(-(\\Sigma^{-1} + \\Sigma^{-T})x_i+(\\Sigma^{-1} + \\Sigma^{-T})\\mu_k) = \\gamma_{ik}\\Sigma^{-1}(x_i-\\mu_k)$$<br>\n",
    "\n",
    "Compare to 0 for all samples:<br>\n",
    "$$\\sum_{i=1}^n\\gamma_{ik}\\Sigma^{-1}(x_i-\\mu_k)=0$$\n",
    "$$\\Leftrightarrow\\sum_{i=1}^n\\gamma_{ik}(x_i-\\mu_k)=0$$\n",
    "$$\\Leftrightarrow\\sum_{i=1}^n\\gamma_{ik}\\mu_k=\\sum_{i=1}^n\\gamma_{ik}x_i$$\n",
    "$$\\Leftrightarrow\\mu_k=\\frac{\\sum_{i=1}^n\\gamma_{ik}x_i}{\\sum_{i=1}^n\\gamma_{ik}}$$\n",
    "\n",
    "$\\Sigma_k$:<br>\n",
    "$$\\nabla_{\\Sigma_k}\\sum_l \\gamma_{il}[log(\\pi_l N(x_i|\\mu_l, \\Sigma_l))]=\\nabla_{\\Sigma_k}\\sum_l \\gamma_{il}[log(\\frac{1}{2\\pi|\\Sigma_l|})-\\frac{1}{2}(x_i-\\mu_l)^T\\Sigma^{-1}(x_i-\\mu_l)] =$$ \n",
    "$$\\nabla_{\\Sigma_k}\\sum_l \\gamma_{il}[log(\\frac{1}{2\\pi})+log(|\\Sigma_l^{-1}|)-\\frac{1}{2}tr[(x_i-\\mu_l)^T\\Sigma^{-1}(x_i-\\mu_l)]] = \\gamma_{ik}[\\nabla_{\\Sigma_k}log(|\\Sigma_k^{-1}|)-\\frac{1}{2}\\nabla_{\\Sigma_k}tr[\\Sigma^{-1}(x_i-\\mu_k)(x_i-\\mu_k)^T]] =$$\n",
    "$$ \\gamma_{ik}[\\Sigma_k^T - (x_i-\\mu_k)(x_i-\\mu_k)^T] = \\gamma_{ik}[\\Sigma_k - (x_i-\\mu_k)(x_i-\\mu_k)^T]$$\n",
    "\n",
    "Compare to 0 for all samples:\n",
    "$$\\sum_{i=1}^n\\gamma_{il}\\left[\\Sigma_k-(x_i-\\mu_k)(x_i-\\mu_k)^T\\right]=0$$\n",
    "$$\\Leftrightarrow\\Sigma_k=\\frac{\\Sigma_{i=1}^n\\gamma_{ik}(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\Sigma_{i=1}^n\\gamma_{ik}}$$\n",
    "\n",
    "$\\pi_k$:<br>\n",
    "for $\\pi_k$ we have an additional constraint: $\\Sigma_l \\pi_l=1$<br>\n",
    "Using Lagrange multiplier:<br>\n",
    "$$\\sum_l \\gamma_{il}[log(\\pi_l N(x_i|\\mu_l, \\Sigma_l))]+\\lambda(1-\\Sigma_l \\pi_l)$$<br>\n",
    "$$\\implies \\nabla_{\\pi_k}\\sum_l \\gamma_{il}[log(\\pi_l) + log(N(x_i|\\mu_l, \\Sigma_l))]+\\lambda(1-\\Sigma_l \\pi_l)=\\frac{\\gamma_{ik}}{\\pi_k}-\\lambda$$\n",
    "\n",
    "Compare to 0:<br>\n",
    "$$\\pi_k=\\frac{\\gamma_{ik}}{\\lambda}$$\n",
    "\n",
    "Find an expression for $\\lambda$ by summing all partial derivatives of $\\pi_k$:<br>\n",
    "$$\\Sigma_l\\pi_l=1 \\implies \\Sigma_l\\frac{\\gamma_{ik}}{\\lambda}=1 \\implies \\lambda=\\Sigma_l\\gamma_{il}=1$$\n",
    "\n",
    "Substituting back in the Lagrangian derivative:<br>\n",
    "$$\\implies \\nabla_{\\pi_k}\\sum_l \\gamma_{il}[log(\\pi_l N(x_i|\\mu_l, \\Sigma_l))]+(1-\\Sigma_l \\pi_l)=\\frac{\\gamma_{ik}}{\\pi_k} - 1$$<br>\n",
    "Summing over all samples and comparing to 0:<br>\n",
    "$$\\Sigma_{i=1}^n(\\frac{\\gamma_{ik}}{\\pi_k}-1)=0 \\implies \\frac{1}{\\pi_k}(\\Sigma_{i=1}^n\\gamma_{ik})-n=0\\Rightarrow\\pi_k=\\frac{\\Sigma_i\\gamma_{ik}}{n}$$\n",
    "\n",
    "For conclusion:<br>\n",
    "$$\\mu_k=\\frac{\\Sigma_{i=1}^n\\gamma_{ik}x_i}{\\Sigma_{i=1}^n\\gamma_{ik}}$$<br>\n",
    "$$\\Sigma_k=\\frac{\\Sigma_{i=1}^n\\gamma_{ik}(x_i-\\mu_k)(x_i-\\mu_k)^T}{\\Sigma_{i=1}^n\\gamma_{ik}}$$<br>\n",
    "$$\\pi_k=\\frac{\\Sigma_i\\gamma_{ik}}{n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBQDjpuigZR1"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ee046202_hw3_034462796_204813323.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
