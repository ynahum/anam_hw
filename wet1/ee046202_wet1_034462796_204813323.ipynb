{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Computer Assignment 1 - Statistics & Dimensionality Reduction\n",
    "\n",
    "### <a style='color:red'> Due Date: 27.12.2020 </a>\n",
    "----\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Statistics\n",
    "    * Descriptive Statistics\n",
    "    * Point Estimation\n",
    "    * Confidence Intervals\n",
    "    * Hypothesis Testing\n",
    "* Dimensionality Reduction\n",
    "    * Robustness to Noise & Outliers\n",
    "    * Comparing Different Methods\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Yair Nahum| nahum.yair@campus.technion.ac.il| 034462796|\n",
    "|David Regev| regev.david@campus.technion.ac.il| 204813323|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100**.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_wet1_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ee046202_wet1_id1_id2.zip` with content:\n",
    "        * `ee046202_wet1_id1_id2.ipynb` - the code tasks\n",
    "        * `ee046202_wet1_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046202-unsupervised-learning-data-analysis) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`wet1` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/info.png\" style=\"height:50px;display:inline\"> Tip\n",
    "---\n",
    "If you find it more convenient, you can copy the section to a new cell, and answer the question or rite the code just right below it. For example:\n",
    "\n",
    "#### Question 0\n",
    "1. What is the best course in the Technion?\n",
    "2. Why does no one pick Bulbasaur as first pokemon?\n",
    "3. Why is there no superhero named Catman?\n",
    "\n",
    "#### Answers - Q0\n",
    "\n",
    "#### Q0 - Section 1\n",
    "* Q: What is the best course in the Technion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANAM!\n"
     ]
    }
   ],
   "source": [
    "print(\"ANAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q0 - Section 2\n",
    "* Q: Why does no one pick Bulbasaur as first pokemon?\n",
    "\n",
    "It is really a riddle....\n",
    "\n",
    "#### Q0 - Section 3\n",
    "* Q: Why is there no superhero named Catman?\n",
    "\n",
    "I got nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/us-capitol.png\" style=\"height:50px;display:inline\"> Part 1 - Statistics -Prologue - US 2016 Elections\n",
    "---\n",
    "In this exercise we are going to analyze the 2016 presidential elections in the United States of America.\n",
    "The data is located in `./election-context-2018.csv`.\n",
    "\n",
    "The complete details of the fields can be found here: https://github.com/MEDSL/2018-elections-unoffical/blob/master/election-context-2018.md\n",
    "\n",
    "**Fields**:\n",
    "* **state** - state name\n",
    "* **county** - county name\n",
    "* **trump16** - presidential candidate (Trump) vote totals\n",
    "* **clinton16** - presidential candidate (Clinton) vote totals\n",
    "* **otherpres16** - presidential candidate (Other) vote totals\n",
    "* **total_population** - total population\n",
    "* **cvap** - citizen voting-age population (or: how many citizens were allowed to vote)\n",
    "* **white_pct** -  non-Hispanic whites as a percentage of *total population*\n",
    "* **black_pct** - non-Hispanic blacks as a percentage of *total population*\n",
    "* **hispanic_pct** - Hispanics or Latinos as a percentage of *total population*\n",
    "* **nonwhite_pct** - non-whites as a percentage of *total population*\n",
    "* **foreignborn_pct** - foreign-born population as a percentage of *total population*\n",
    "* **female_pct** - females as a percentage of *total population*\n",
    "* **median_hh_inc** - *median* household income in the past 12 months\n",
    "* **clf_unemploy_pct** - unemployed population in labor force as a percentage of total population in civilian labor force\n",
    "* **lesshs_pct** - population with an education of less than a regular high school diploma as a percentage of total\n",
    "* **lesscollege_pct** - population with an education of less than a bachelor's degree as a percentage of total population\n",
    "\n",
    "Let's have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the exrcise - part 1\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>trump16</th>\n",
       "      <th>clinton16</th>\n",
       "      <th>otherpres16</th>\n",
       "      <th>total_population</th>\n",
       "      <th>cvap</th>\n",
       "      <th>white_pct</th>\n",
       "      <th>black_pct</th>\n",
       "      <th>hispanic_pct</th>\n",
       "      <th>nonwhite_pct</th>\n",
       "      <th>foreignborn_pct</th>\n",
       "      <th>female_pct</th>\n",
       "      <th>median_hh_inc</th>\n",
       "      <th>clf_unemploy_pct</th>\n",
       "      <th>lesshs_pct</th>\n",
       "      <th>lesscollege_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>18172</td>\n",
       "      <td>5936</td>\n",
       "      <td>865</td>\n",
       "      <td>55049.0</td>\n",
       "      <td>40690.0</td>\n",
       "      <td>75.683482</td>\n",
       "      <td>18.370906</td>\n",
       "      <td>2.572254</td>\n",
       "      <td>24.316518</td>\n",
       "      <td>1.838362</td>\n",
       "      <td>51.176225</td>\n",
       "      <td>53099.0</td>\n",
       "      <td>5.591657</td>\n",
       "      <td>12.417046</td>\n",
       "      <td>75.407229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>72883</td>\n",
       "      <td>18458</td>\n",
       "      <td>3874</td>\n",
       "      <td>199510.0</td>\n",
       "      <td>151770.0</td>\n",
       "      <td>83.178788</td>\n",
       "      <td>9.225603</td>\n",
       "      <td>4.366698</td>\n",
       "      <td>16.821212</td>\n",
       "      <td>3.269510</td>\n",
       "      <td>51.194928</td>\n",
       "      <td>51365.0</td>\n",
       "      <td>6.286843</td>\n",
       "      <td>9.972418</td>\n",
       "      <td>70.452889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>5454</td>\n",
       "      <td>4871</td>\n",
       "      <td>144</td>\n",
       "      <td>26614.0</td>\n",
       "      <td>20375.0</td>\n",
       "      <td>45.885624</td>\n",
       "      <td>47.888329</td>\n",
       "      <td>4.309762</td>\n",
       "      <td>54.114376</td>\n",
       "      <td>2.859397</td>\n",
       "      <td>46.498084</td>\n",
       "      <td>33956.0</td>\n",
       "      <td>12.824738</td>\n",
       "      <td>26.235928</td>\n",
       "      <td>87.132213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>6738</td>\n",
       "      <td>1874</td>\n",
       "      <td>207</td>\n",
       "      <td>22572.0</td>\n",
       "      <td>17590.0</td>\n",
       "      <td>74.765196</td>\n",
       "      <td>21.212121</td>\n",
       "      <td>2.223994</td>\n",
       "      <td>25.234804</td>\n",
       "      <td>1.351232</td>\n",
       "      <td>46.464646</td>\n",
       "      <td>39776.0</td>\n",
       "      <td>7.146827</td>\n",
       "      <td>19.301587</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>22859</td>\n",
       "      <td>2156</td>\n",
       "      <td>573</td>\n",
       "      <td>57704.0</td>\n",
       "      <td>42430.0</td>\n",
       "      <td>87.657701</td>\n",
       "      <td>1.557951</td>\n",
       "      <td>8.727298</td>\n",
       "      <td>12.342299</td>\n",
       "      <td>4.271801</td>\n",
       "      <td>50.485235</td>\n",
       "      <td>46212.0</td>\n",
       "      <td>5.953833</td>\n",
       "      <td>19.968585</td>\n",
       "      <td>86.950243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Sweetwater</td>\n",
       "      <td>12154</td>\n",
       "      <td>3231</td>\n",
       "      <td>1745</td>\n",
       "      <td>44812.0</td>\n",
       "      <td>30565.0</td>\n",
       "      <td>79.815674</td>\n",
       "      <td>0.865840</td>\n",
       "      <td>15.859591</td>\n",
       "      <td>20.184326</td>\n",
       "      <td>5.509685</td>\n",
       "      <td>47.824244</td>\n",
       "      <td>68233.0</td>\n",
       "      <td>5.072255</td>\n",
       "      <td>9.314606</td>\n",
       "      <td>78.628507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton</td>\n",
       "      <td>3921</td>\n",
       "      <td>7314</td>\n",
       "      <td>1392</td>\n",
       "      <td>22623.0</td>\n",
       "      <td>16335.0</td>\n",
       "      <td>81.200548</td>\n",
       "      <td>0.614419</td>\n",
       "      <td>15.174822</td>\n",
       "      <td>18.799452</td>\n",
       "      <td>11.475048</td>\n",
       "      <td>48.097069</td>\n",
       "      <td>75594.0</td>\n",
       "      <td>2.123447</td>\n",
       "      <td>4.633570</td>\n",
       "      <td>46.211584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Uinta</td>\n",
       "      <td>6154</td>\n",
       "      <td>1202</td>\n",
       "      <td>1114</td>\n",
       "      <td>20893.0</td>\n",
       "      <td>14355.0</td>\n",
       "      <td>87.718375</td>\n",
       "      <td>0.186665</td>\n",
       "      <td>8.959939</td>\n",
       "      <td>12.281625</td>\n",
       "      <td>3.986981</td>\n",
       "      <td>49.327526</td>\n",
       "      <td>53323.0</td>\n",
       "      <td>6.390755</td>\n",
       "      <td>10.361224</td>\n",
       "      <td>81.793082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>2911</td>\n",
       "      <td>532</td>\n",
       "      <td>371</td>\n",
       "      <td>8351.0</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>82.397318</td>\n",
       "      <td>0.790325</td>\n",
       "      <td>13.962400</td>\n",
       "      <td>17.602682</td>\n",
       "      <td>3.783978</td>\n",
       "      <td>51.359119</td>\n",
       "      <td>46212.0</td>\n",
       "      <td>7.441860</td>\n",
       "      <td>12.577108</td>\n",
       "      <td>78.923920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>3033</td>\n",
       "      <td>299</td>\n",
       "      <td>194</td>\n",
       "      <td>7175.0</td>\n",
       "      <td>5565.0</td>\n",
       "      <td>92.222997</td>\n",
       "      <td>0.250871</td>\n",
       "      <td>1.003484</td>\n",
       "      <td>7.777003</td>\n",
       "      <td>4.083624</td>\n",
       "      <td>47.958188</td>\n",
       "      <td>55640.0</td>\n",
       "      <td>3.610949</td>\n",
       "      <td>8.592392</td>\n",
       "      <td>81.193281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3114 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        state      county  trump16  clinton16  otherpres16  total_population  \\\n",
       "0     Alabama     Autauga    18172       5936          865           55049.0   \n",
       "1     Alabama     Baldwin    72883      18458         3874          199510.0   \n",
       "2     Alabama     Barbour     5454       4871          144           26614.0   \n",
       "3     Alabama        Bibb     6738       1874          207           22572.0   \n",
       "4     Alabama      Blount    22859       2156          573           57704.0   \n",
       "...       ...         ...      ...        ...          ...               ...   \n",
       "3109  Wyoming  Sweetwater    12154       3231         1745           44812.0   \n",
       "3110  Wyoming       Teton     3921       7314         1392           22623.0   \n",
       "3111  Wyoming       Uinta     6154       1202         1114           20893.0   \n",
       "3112  Wyoming    Washakie     2911        532          371            8351.0   \n",
       "3113  Wyoming      Weston     3033        299          194            7175.0   \n",
       "\n",
       "          cvap  white_pct  black_pct  hispanic_pct  nonwhite_pct  \\\n",
       "0      40690.0  75.683482  18.370906      2.572254     24.316518   \n",
       "1     151770.0  83.178788   9.225603      4.366698     16.821212   \n",
       "2      20375.0  45.885624  47.888329      4.309762     54.114376   \n",
       "3      17590.0  74.765196  21.212121      2.223994     25.234804   \n",
       "4      42430.0  87.657701   1.557951      8.727298     12.342299   \n",
       "...        ...        ...        ...           ...           ...   \n",
       "3109   30565.0  79.815674   0.865840     15.859591     20.184326   \n",
       "3110   16335.0  81.200548   0.614419     15.174822     18.799452   \n",
       "3111   14355.0  87.718375   0.186665      8.959939     12.281625   \n",
       "3112    6135.0  82.397318   0.790325     13.962400     17.602682   \n",
       "3113    5565.0  92.222997   0.250871      1.003484      7.777003   \n",
       "\n",
       "      foreignborn_pct  female_pct  median_hh_inc  clf_unemploy_pct  \\\n",
       "0            1.838362   51.176225        53099.0          5.591657   \n",
       "1            3.269510   51.194928        51365.0          6.286843   \n",
       "2            2.859397   46.498084        33956.0         12.824738   \n",
       "3            1.351232   46.464646        39776.0          7.146827   \n",
       "4            4.271801   50.485235        46212.0          5.953833   \n",
       "...               ...         ...            ...               ...   \n",
       "3109         5.509685   47.824244        68233.0          5.072255   \n",
       "3110        11.475048   48.097069        75594.0          2.123447   \n",
       "3111         3.986981   49.327526        53323.0          6.390755   \n",
       "3112         3.783978   51.359119        46212.0          7.441860   \n",
       "3113         4.083624   47.958188        55640.0          3.610949   \n",
       "\n",
       "      lesshs_pct  lesscollege_pct  \n",
       "0      12.417046        75.407229  \n",
       "1       9.972418        70.452889  \n",
       "2      26.235928        87.132213  \n",
       "3      19.301587        88.000000  \n",
       "4      19.968585        86.950243  \n",
       "...          ...              ...  \n",
       "3109    9.314606        78.628507  \n",
       "3110    4.633570        46.211584  \n",
       "3111   10.361224        81.793082  \n",
       "3112   12.577108        78.923920  \n",
       "3113    8.592392        81.193281  \n",
       "\n",
       "[3114 rows x 17 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/election-context-2018.csv')\n",
    "df = df[['state', 'county','trump16', 'clinton16', 'otherpres16','total_population',\n",
    "         'cvap', 'white_pct', 'black_pct', 'hispanic_pct', 'nonwhite_pct', 'foreignborn_pct',\n",
    "         'female_pct', 'median_hh_inc', 'clf_unemploy_pct', 'lesshs_pct', 'lesscollege_pct']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - Preprocessing\n",
    "---\n",
    "We don't actually care about the counties, and thus we will perform the following preprocessing steps:\n",
    "* For each state, remove counties that has NaN in at least one of their fields (use `df.dropna()`)\n",
    "* For each state, *sum* all the countable fields (votes, total population, cvap...)\n",
    "* For each state, first change percentage fields (*_pct*) to numbers, and then *sum* for each state and finally re-calculate the percentage out of the new total population. Note that the percentage is out of the *total_population*.\n",
    "* For each state, calculate the *weighted median* of the the household median and then calculate the mean (average of medians)\n",
    "    * We want the household median for each state, and in order to that we need to compute the weighted median of each county ($median * \\text{total_population}$) and then the best we can do (as we don't that data available) is calculate the mean.\n",
    "* Add a new column called `trump16_pct`- the precentage of votes Trump got in each state.\n",
    "* You should end up with a `DataFrame` similar to the original (cell above), with the extra column, but with less rows. The name of the new DataFrame should be `prep_df` instead of `df`.\n",
    "\n",
    "**Hints**: `pandas` has great tools for doing all of this. Check out: `df[df['state']=='Alabama']`, `df.uniuqe()`, `df.sum()`, `df.append()`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>median_hh_inc</th>\n",
       "      <th>total_population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>45708.307187</td>\n",
       "      <td>4841164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>51693.661933</td>\n",
       "      <td>6728577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>43195.051075</td>\n",
       "      <td>2968472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>65366.512280</td>\n",
       "      <td>38654206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>63927.379147</td>\n",
       "      <td>5359295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>72706.808337</td>\n",
       "      <td>3588570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>61520.368889</td>\n",
       "      <td>934695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>72935.000000</td>\n",
       "      <td>659009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Florida</td>\n",
       "      <td>49050.777091</td>\n",
       "      <td>19934451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>52480.766171</td>\n",
       "      <td>10099320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>72569.944127</td>\n",
       "      <td>1413582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>49496.785762</td>\n",
       "      <td>1635483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>60574.045850</td>\n",
       "      <td>12851684.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>51379.548135</td>\n",
       "      <td>6589578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>55214.437196</td>\n",
       "      <td>3106589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>55357.440233</td>\n",
       "      <td>2898292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>45959.049651</td>\n",
       "      <td>4411989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>46248.258200</td>\n",
       "      <td>4645670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maine</td>\n",
       "      <td>51456.529291</td>\n",
       "      <td>1329923.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>78895.885257</td>\n",
       "      <td>5959902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>72391.208034</td>\n",
       "      <td>6742143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>51729.191722</td>\n",
       "      <td>9909600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>64828.628493</td>\n",
       "      <td>5450868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>41085.151999</td>\n",
       "      <td>2989192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>50748.268840</td>\n",
       "      <td>6059651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Montana</td>\n",
       "      <td>48641.952293</td>\n",
       "      <td>1023391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>55183.661282</td>\n",
       "      <td>1881259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>53275.583685</td>\n",
       "      <td>2839172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>69517.375899</td>\n",
       "      <td>1327503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>74686.161738</td>\n",
       "      <td>8915456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>46241.300114</td>\n",
       "      <td>2082669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New York</td>\n",
       "      <td>64021.335279</td>\n",
       "      <td>19697457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>49484.633254</td>\n",
       "      <td>9940828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>59707.089781</td>\n",
       "      <td>736162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>51449.175006</td>\n",
       "      <td>11586941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>48501.169114</td>\n",
       "      <td>3875589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>54625.576106</td>\n",
       "      <td>3982267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>56751.761557</td>\n",
       "      <td>12783977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>58483.276409</td>\n",
       "      <td>1054491.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>47359.625661</td>\n",
       "      <td>4834605.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>52915.994695</td>\n",
       "      <td>836795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>47969.446660</td>\n",
       "      <td>6548009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Texas</td>\n",
       "      <td>55916.854723</td>\n",
       "      <td>26956435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Utah</td>\n",
       "      <td>63068.939984</td>\n",
       "      <td>2948427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>56866.504055</td>\n",
       "      <td>626249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>72743.830072</td>\n",
       "      <td>8310301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Washington</td>\n",
       "      <td>64487.898623</td>\n",
       "      <td>7073146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>43318.276973</td>\n",
       "      <td>1846092.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>55672.932335</td>\n",
       "      <td>5754798.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>59863.696818</td>\n",
       "      <td>583029.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   state  median_hh_inc  total_population\n",
       "0                Alabama   45708.307187         4841164.0\n",
       "1                Arizona   51693.661933         6728577.0\n",
       "2               Arkansas   43195.051075         2968472.0\n",
       "3             California   65366.512280        38654206.0\n",
       "4               Colorado   63927.379147         5359295.0\n",
       "5            Connecticut   72706.808337         3588570.0\n",
       "6               Delaware   61520.368889          934695.0\n",
       "7   District of Columbia   72935.000000          659009.0\n",
       "8                Florida   49050.777091        19934451.0\n",
       "9                Georgia   52480.766171        10099320.0\n",
       "10                Hawaii   72569.944127         1413582.0\n",
       "11                 Idaho   49496.785762         1635483.0\n",
       "12              Illinois   60574.045850        12851684.0\n",
       "13               Indiana   51379.548135         6589578.0\n",
       "14                  Iowa   55214.437196         3106589.0\n",
       "15                Kansas   55357.440233         2898292.0\n",
       "16              Kentucky   45959.049651         4411989.0\n",
       "17             Louisiana   46248.258200         4645670.0\n",
       "18                 Maine   51456.529291         1329923.0\n",
       "19              Maryland   78895.885257         5959902.0\n",
       "20         Massachusetts   72391.208034         6742143.0\n",
       "21              Michigan   51729.191722         9909600.0\n",
       "22             Minnesota   64828.628493         5450868.0\n",
       "23           Mississippi   41085.151999         2989192.0\n",
       "24              Missouri   50748.268840         6059651.0\n",
       "25               Montana   48641.952293         1023391.0\n",
       "26              Nebraska   55183.661282         1881259.0\n",
       "27                Nevada   53275.583685         2839172.0\n",
       "28         New Hampshire   69517.375899         1327503.0\n",
       "29            New Jersey   74686.161738         8915456.0\n",
       "30            New Mexico   46241.300114         2082669.0\n",
       "31              New York   64021.335279        19697457.0\n",
       "32        North Carolina   49484.633254         9940828.0\n",
       "33          North Dakota   59707.089781          736162.0\n",
       "34                  Ohio   51449.175006        11586941.0\n",
       "35              Oklahoma   48501.169114         3875589.0\n",
       "36                Oregon   54625.576106         3982267.0\n",
       "37          Pennsylvania   56751.761557        12783977.0\n",
       "38          Rhode Island   58483.276409         1054491.0\n",
       "39        South Carolina   47359.625661         4834605.0\n",
       "40          South Dakota   52915.994695          836795.0\n",
       "41             Tennessee   47969.446660         6548009.0\n",
       "42                 Texas   55916.854723        26956435.0\n",
       "43                  Utah   63068.939984         2948427.0\n",
       "44               Vermont   56866.504055          626249.0\n",
       "45              Virginia   72743.830072         8310301.0\n",
       "46            Washington   64487.898623         7073146.0\n",
       "47         West Virginia   43318.276973         1846092.0\n",
       "48             Wisconsin   55672.932335         5754798.0\n",
       "49               Wyoming   59863.696818          583029.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here - you can use as many cells as you need\n",
    "#1.1\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#1.2\n",
    "countables_feilds = ['state', 'trump16', 'clinton16', 'otherpres16','total_population', 'cvap']\n",
    "countables_df = df[countables_feilds]\n",
    "state_countable_df_sums = countables_df.groupby('state').sum().reset_index()\n",
    "#display(state_countable_df_sums)\n",
    "\n",
    "#1.3\n",
    "pct_feilds = ['white_pct', 'black_pct', 'hispanic_pct', 'nonwhite_pct', 'foreignborn_pct',\n",
    "    'female_pct', 'clf_unemploy_pct', 'lesshs_pct', 'lesscollege_pct']\n",
    "pct_df = df[pct_feilds]\n",
    "#display(pct_df)\n",
    "total_pop = df[['total_population']].values\n",
    "#display(total_pop)\n",
    "porportions_df = pct_df * total_pop / 100\n",
    "#display(counties_porps_df)\n",
    "porportions_df[['state','total_population']] = df[['state','total_population']]\n",
    "state_porportions_df = porportions_df.groupby('state').sum().reset_index()\n",
    "#display(state_porportions_df)\n",
    "state_pct_df = state_porportions_df\n",
    "state_pct_df = state_porportions_df[pct_feilds] * 100 / state_porportions_df[['total_population']].values\n",
    "#display(state_pct_df)\n",
    "\n",
    "#1.4\n",
    "hh_median_feild = ['median_hh_inc']\n",
    "hh_median_df = df[hh_median_feild]\n",
    "county_weighted_hh_median_df = hh_median_df * total_pop\n",
    "#display(county_weighted_hh_median_df)\n",
    "county_weighted_hh_median_df[['state','total_population']] = df[['state','total_population']]\n",
    "state_mean_hh_median_df = county_weighted_hh_median_df.groupby('state').sum().reset_index()\n",
    "state_mean_hh_median_df['median_hh_inc'] /= state_mean_hh_median_df['total_population']\n",
    "#display(state_mean_hh_median_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - Descriptive Statistics\n",
    "---\n",
    "Run the following command (code cell below): `fig = sns.pairplot(data=prep_df[['trump16_pct', 'median_hh_inc', 'lesshs_pct', 'black_pct', 'white_pct']])` and answer:\n",
    "\n",
    "- Explain, in general, what does this plot show.\n",
    "- What are the points on the graphs?\n",
    "- Identify interesting trends.\n",
    "- If the winner of the elections was decided based purely in the number of voters fot each candidate, who would have won the elections in 2016? (Hint: use `prep_df.sum()`)\n",
    "\n",
    "\n",
    "Note: please refrain from expressing personal opinion, we are merely analyzing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(data=prep_df[['trump16_pct', 'median_hh_inc', 'lesshs_pct', 'black_pct', 'white_pct']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - Point Estimates\n",
    "---\n",
    "As learned in class, point estimates are estimates of population parameters based on sample data. For instance, if we wanted to know the average age of registered voters in the U.S., we could take a survey of registered voters and then use the average age of the respondents as a point estimate of the average age of the population as a whole. The average of a sample is known as the sample mean. The sample mean is usually not exactly the same as the population mean. This difference can be caused by many factors including poor survey design, biased sampling methods and the randomness inherent to drawing a sample from a population\n",
    "\n",
    "In this task, we will go back to the original data, `df` in the code, and we will treat each *county* as one vote and we will try to estimate the **mean** *median income* of voters.\n",
    "\n",
    "* Constant the seed (`np.random.seed(0)`)\n",
    "* Clean the original data by removing counties with NaN (as before, use `df.dropna()`)\n",
    "* Calculate the **true** mean of the *median income* (print it)\n",
    "* Take a random sample of $n=1000$ samples, calculate the sample mean, and calculate the difference from the true mean (print them)\n",
    "* Repeat the process of sampling $n=1000$ (with replacement) $N=10, 20, 40, 50, 100, 200, 500, 1000, 2000$ times, and for each $N$ calculate the mean, calculate the difference from the true mean, plot the difference from the true mean vs. $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Point Estimates\n",
    "---\n",
    "* What is the process of sampling that you just performed? Explain briefly how it works and what is it good for.\n",
    "* What will happen as $n \\to \\infty$? Refer to the Central Limit Theorem (CLT) in you answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Confidence Intervals\n",
    "---\n",
    "A point estimate can give you a rough idea of a population parameter like the mean, but estimates are prone to error and taking multiple samples to get improved estimates may not be feasible (would you call 500 people, 2000 times?). A confidence interval is a range of values above and below a point estimate that captures the true population parameter at some predetermined confidence level. For example, if you want to have a 95% chance of capturing the true population parameter with a point estimate and a corresponding confidence interval, you'd set your confidence level to 95%. Higher confidence levels result in a wider confidence intervals.\n",
    "\n",
    "Calculating a confidence interval -  taking a point estimate and then adding and subtracting a margin of error to create a range. Margin of error is based on your desired confidence level, the spread of the data and the size of your sample. The way you calculate the margin of error depends on whether you know the standard deviation of the population or not.\n",
    "\n",
    "* Calculate a 90%, 95% and a 99% confidence interval for the **mean** *median income* of voters. Take a sample of $n=1000$.\n",
    "    * Equivalently, $\\alpha=[0.1, 0.05, 0.01]$\n",
    "    * Don't forget to **standartize** the data. and notice that $\\sigma \\neq 1$ as in the tutorial.\n",
    "    * Use the true STD of the population\n",
    "    * To calculate the inverse of the CDF, use `scipy.stats.norm.ppf()`\n",
    "* Compare your answer to using `scipy.stats.norm.interval(alpha=, loc=, scale=)`. Read the doc, use Shift + Tab (only in Jupyter) inside the parenthesis to understand how to use it.\n",
    "    * You need to print the result for each $\\alpha$, with both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Confidence Intervals\n",
    "---\n",
    "* Explain the results and their meaning.\n",
    "    * What is the trend (the higher the confidence...)?\n",
    "* Why point estimation is not enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4 - Confidence Intervals 2\n",
    "---\n",
    "- Take $N=25$ samples of $n=1000$, and plot the confidence interval for each sample (x axis - sample #, y axis - confidence interval for a confidence level of 95% ($\\alpha=0.05$)\n",
    "    * Use `ax.errorbar(x=list(range(N)), y=sample_mean, yerr=[(upper-lower)/2 for upper,lower in intervals], fmt='o')`\n",
    "    * Add the **true** mean using `ax.hlines((xmin=0, xmax=N, y=true_mean, linewidth=2.0, color=\"red\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 4 - Confidence Intervals 2\n",
    "---\n",
    "- As you have probably noticed, there intervals that **don't include** the true mean. How is that possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 5 - Hypothesis Testing - Two-Sided One Sample T-Test\n",
    "---\n",
    "Point estimates and confidence intervals are basic inference tools that form the foundation for another inference technique: statistical hypothesis testing. Statistical hypothesis testing is a framework for determining whether observed data deviates from what is expected. The T-test is a statistical test used to determine whether a numeric data sample differs significantly from the population or whether two samples differ from one another.\n",
    "\n",
    "In this exercise we will perform the t-test for the mean of unemployment fraction (`clf_unemploy_pct`) in the US. We will use the pre-procesesd data (`prep_df`) and a sample of size $n=10$. We wish to test whether the sample mean differs from the population mean. The null hypothesis, $H_0$ is that the mean is not different. Recall that we are performng t-tests when we are not given the true mean and true STD. The steps:\n",
    "\n",
    "* Constant the seed\n",
    "* Calculate the true mean of unemployment fraction (use fractions and not percents: `clf_unemploy_pct / 100`)\n",
    "    * In t-test, you assume that the mean ($\\mu$) is unknown, then you formulate the $H_0: \\mu= \\mu_0$. So if $\\mu$ is unknown, how do you calculate the t-statistic? Well, assume that somebody told you that in the last elections, $\\mu = C$, where $C$ is the also the true mean of this election.\n",
    "    * In other words, to calculate t-statistic, use the true mean. \n",
    "* Take a sample of $n=10$ and calculate its mean.\n",
    "* Calculate the sample's unbiased STD.\n",
    "* Calculate the t-statistic value.\n",
    "    * Use the true mean for $\\overline{\\mu} - \\mu_0$\n",
    "* Calculate $t_{n-1, \\frac{\\alpha}{2}}$ for $\\alpha=[0.1, 0.05, 0.01]$ significance levels (how many derees of freedom are there for the t-distribution?). Use `scipy.stats.t.ppf()`.\n",
    "* Calculate the p-value (don't forget that this is a **two-sided** test). For each level, do we reject $H_0$ or accept it?. Use `scipy.stats.t.cdf()`.\n",
    "* Compare your results to `scipy.stats.ttest_1samp()`. This function performs the t-test, and outputs the t-statistic and the p-value.\n",
    "\n",
    "Note: \"Calculate\" = print the results\n",
    "\n",
    "* Tips:\n",
    "    * When calculating the p-value, pay attention to which side are you on (if t-statitic > 0, then you should calculate the p-value for $1-\\phi(x)$)\n",
    "    * Your results should align with the output of `scipy.stats.ttest_1samp()` (make sure the seed is constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 5 - Hypothesis Testing - Two-Sided One Sample T-Test\n",
    "---\n",
    "* For a sample of size $n$, what is the number of degrees of freedom for the student-t distribution?\n",
    "* What is the relationship between the significance level and the confidence level? Explain.\n",
    "* What is the meaning of the p-value? What is the meaning of the p-value being lower than some siginificance level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/wine-glass.png\" style=\"height:50px;display:inline\"> Part 2 - Dimensionality Reduction - Prologue - Wine Dataset\n",
    "---\n",
    "In this exercise we are going to compare different dimensionality reduction techniques on the Wine dataset.\n",
    "The wine dataset is a classic and very easy multi-class classification dataset. There are 3 types of wine, 178 examples with 13 features each. Even though it is a very dataset for classification, we will do unsupervised analysis to compare different aspects of dimensionality reduction methods. Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the exrcise - part 2\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X, y = load_wine(return_X_y=True)\n",
    "pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=list(range(13)) + ['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 6 - Importance of Feature Scaling\n",
    "---\n",
    "* Perform PCA on the data (`X`) to `n_components=2` and plot it.\n",
    "* Scale the data using `StandardSaler()` to create `X_scaled`, perform PCA on `X_scaled` to `n_components=2` and plot it.\n",
    "* Color the datapoints according to their class: `ax.scatter(X[:,0], X[:,1], c=y)`\n",
    "\n",
    "Note: for all algorithms, use constant seed (`random_state=...`), for example: `PCA(n_component=2, random_state=random_state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 6 - Importance of Feature Scaling\n",
    "---\n",
    "- Why is it so important to perform feature scaling before performing dimensionality reduction, espicially for PCA?\n",
    "- Describe the results of Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 7 - T-SNE\n",
    "---\n",
    "* Perform t-SNE on the scaled dataset to `n_components=2` and plot it.\n",
    "    * Find the `perplexity`, the yields the best-looking results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 7 - T-SNE\n",
    "---\n",
    "- Explain in short how t-SNE works and how it is different from PCA.\n",
    "- Compare the results of PCA and t-SNE on the Wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 8 - A Short Visit to Supervised Learning\n",
    "---\n",
    "In order to evaluate the quality of our dimensionality reduction, we can, given labels, train a classifier. Though this is the focus of ML course, we will briefly train a linear Perceptron on the lower-dimension dataset (multi-class Perceptron is trained using \"One-vs-All\" scheme, but it is not important).\n",
    "\n",
    "* Split the lower-dimension scaled features to train and test test using `train_test_split`. Use 20% of the data for test.\n",
    "    * Usage: `train_test_split(X_scaled_pca/tsne, y, test_size=0.2, random_state=42)`\n",
    "* Train a Perceptron on the scaled PCA dataset, evaluate the accuracy on the test set (print the results).\n",
    "* Train a Perceptron on the scaled t-SNE dataset, evaluate the accuracy on the test set (print the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 8 - A Short Visit to Supervised Learning\n",
    "---\n",
    "- Describe and explain what is Perceptron doing.\n",
    "- Describe the accuracy results. If there is a difference, explain its source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 9 - Robustness to Noise & Outliers\n",
    "---\n",
    "In this task we are going to test how robust are t-SNE and PCA to noisy features and outliers. We will have 2 new datasets:\n",
    "1. `X_noisy` - random normal noise ($\\mathcal{N}(0,1)$) is added to the current features.\n",
    "2. `X_skewed` - 20 random samples are added to the sample (total samples: 178 + 20 = 198)\n",
    "\n",
    "The tasks:\n",
    "* Perform standardization to create `X_noisy_scaled`, `X_skewed_scaled`\n",
    "* Peform PCA and t-SNE to both datasets (4 in total) to `n_components=2` (as before) and plot the results (4 plots, can be in pairs and can be a 2X2 plot, don't forget to put titles).\n",
    "    * You should tune the `perplexity` for t-SNE\n",
    "* For each of: `X_noisy_scaled_pca`, `X_noisy_scaled_tsne`, `X_skewed_scaled_pca`, `X_skewed_scaled_tsne` train a `LogisticRegression` classifier and measure the accuracy (4 classifiers in total). Don't forget to split to train and test, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 9 - Robustness to Noise & Outliers\n",
    "---\n",
    "- Explain the results. In your answer, describe the effect of adding noise and outliers, which algorithm is more affected and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 10 - Comparing Dimensionality Reduction Methods\n",
    "---\n",
    "In this task we will compare different dimensionality reduction techniques on the Digits dataset (Each datapoint is a 8x8 image of a digit). Reduce dimensions to `n_components=2`. Plot all the results. You should tune each algorithm's hyper-parameters. Use the table below:\n",
    "\n",
    "    \n",
    "|Algorithm| Call to...| Hyper-parameters to tune|\n",
    "|---------|-----------|-------------------------|\n",
    "| PCA     | `PCA()`   | None                    |\n",
    "| KernelPCA | `KernelPCA()`   | `kernel`        |\n",
    "| t-SNE     |  `TSNE()`       | `perplexity`    |\n",
    "| Locally Linear Embedding (LLE)|  `LocallyLinearEmbedding()`| `n_neighbors` | \n",
    "| Isomap| Isomap() |`n_neighbors` |\n",
    "\n",
    "* Note, to add a color bar to your plot, use:\n",
    "`scatter = ax.scatter(X_tsne[:,0], X_tsne[:,1], c=y, cmap=plt.cm.rainbow)`\n",
    "`plt.colorbar(scatter)`\n",
    "* Don't forget to add titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 10 - Comparing Dimensionality Reduction Methods\n",
    "---\n",
    "* Shortly describe what each method from the above table does, and the effect of the hyper-parmeters.\n",
    "* Which method produced the most satisfying results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
