{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Homework 1 - Statistics\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: 29.11.2020 </a>\n",
    "\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Questions\n",
    "    * Gaussian RVs\n",
    "    * Parametric & Non-Parametric Estimation\n",
    "    * Exam Question - Estimators\n",
    "* Python Exercise - Parkinson's Disease Classification Data Analysis\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Yair Nahum| nahum.yair@campus.technion.ac.il| 034462796|\n",
    "|David Regev| regev.david@campus.technion.ac.il| 204813323|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
    "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
    "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **ANSWERS TO THEORETICAL/MATHEMATICAL QUESTIONS**:\n",
    "    * **Typed - 5 points bonus**: you can type directly in a Markdown cell using Latex (see cheatsheets above), or use Word, Overleaf, LyX...\n",
    "        * This is a really good practice, we encourage you to practice your math typing skills.\n",
    "    * **Handwritten** - if we can't read your handwriting, we will automatically take off the points of the questions. Please write clearly. No bonus for handwritten submissions.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_hw1_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ee046202_hw1_id1_id2.zip` with content:\n",
    "        * `ee046202_hw1_id1_id2.ipynb` - the code tasks\n",
    "        * `ee046202_hw1_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046202-unsupervised-learning-data-analysis) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw1` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - Gaussian RV- Basics\n",
    "---\n",
    "1. Let $Z \\sim \\mathcal{N}(0,1)$ be a normal Gaussian RV, and $X \\sim \\mathcal{N}(\\mu, \\sigma)$. The Cumulative Distribution Function (CDF) of $Z$ is defined as $$ P(Z\\leq c) \\triangleq \\phi(c).$$ Express $P(X\\geq x)$ using $\\phi(c)$.\n",
    "2. Consider a sequence of $N$ i.i.d. RVs $\\{X_i\\}_{i=1}^N$, where $X_i \\sim \\mathcal{N}(10,1)$. The empirical mean is given by $\\overline{X}_N = \\frac{1}{N}\\sum_{i=1}^NX_i$. What is the distribution of $\\overline{X}_N$?\n",
    "3. What is the probability $P(9.7 \\leq \\overline{X}_N\\leq 10.3)$ for $N=1,10,20$? Express first using the function $\\phi(X)$ and then use `scipy.stats.norm.cdf` to calculate $\\phi(x)$ and obtain a numerical value.\n",
    "4. Since Gaussian RVs are not bounded, we cannot use **Hoeffding's** inequality to bound terms of the form  $P(9.7 \\leq \\overline{X}_N\\leq 10.3)$. A possible alternative for this is to use the following proposition (which is more general and holds for a sum of sub-Gaussian RVs):\n",
    "    * **Proposition**: Let $\\{X_i\\}_{i=1}^N$ be i.i.d. RVs with $X_i \\sim \\mathcal{N}(\\mu, \\sigma)$. Then: $$ P(|\\frac{1}{N}\\sum_{i=1}^NX_i -\\mu|\\geq \\epsilon) \\leq 2 \\exp(-\\frac{N\\epsilon^2}{2\\sigma^2}).$$ Use this proposition to find a lower bound for $P(9.7 \\leq \\overline{X}_N\\leq 10.3)$ for $N=1,10,20$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Answer 1\n",
    "1. We shall normalize/standardize the Gaussian RV X by shift with $\\mu$ and division by $\\sigma$. This transformation still keeps the Gauusian RV to be distributed as Gaussian as any linear transformation on a Gaussian RV is a Gaussian RV:\n",
    "\n",
    "    $$P(X\\geq x) = 1 - P(X\\leq x) = 1 - P(X-\\mu\\leq x-\\mu) = 1 - P(\\frac{X-\\mu}{\\sigma}\\leq \\frac{x-\\mu}{\\sigma}) = 1 - P(Z\\leq \\frac{x-\\mu}{\\sigma}) = 1 - \\phi(\\frac{x-\\mu}{\\sigma})$$\n",
    "    \n",
    "2. The $\\overline{X}_N$ is a linear transformation of n i.i.d. RVs. Thus, it is also distributed as Gaussian RV (it's actually defines a Guassian vector due to i.i.d. The first two moments defines completely the distribution of a normal distributed RV. let's calculate:\n",
    "\n",
    "    $$\\mathbb{E}\\overline{X}_N = \\frac{1}{N}\\sum_{i=1}^N\\mathbb{E}X_i = \\frac{N}{N}\\mathbb{E}X_i = \\mu$$\n",
    "\n",
    "    When first equality is due to expectation linearity\n",
    "    \n",
    "    $$Var(\\overline{X}_N) = \\frac{1}{N^2}Var(\\sum_{i=1}^N X_i) = \\frac{1}{N^2}\\sum_{i=1}^N Var(X_i) = \\frac{\\sigma^2}{N}$$\n",
    "    \n",
    "    When first equality is due to $Var$ property of $Var(\\alpha X) = \\alpha^2 Var(X)$ and the second one is due to $Var$ property on i.i.d RVs.\n",
    "    <br>To conclude: $\\overline{X}_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{N}) = \\mathcal{N}(10, \\frac{1}{N})$\n",
    "    \n",
    "3. As we saw in previous sections, $\\overline{X}_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{N})$ and $\\sqrt{N}\\frac{\\overline{X}_N-\\mu}{\\sigma} \\sim \\mathcal{N}(0,1)$:\n",
    " \n",
    "    $$P(9.7 \\leq \\overline{X}_N\\leq 10.3) = \n",
    "    P(\\overline{X}_N\\leq 10.3) + P(9.7 \\leq \\overline{X}_N) - 1 = \n",
    "    P(\\overline{X}_N\\leq 10.3) - P(\\overline{X}_N \\leq 9.7)$$\n",
    "    \n",
    "    The above is since $P(A\\cap B) = P(A) + P(B) - P(A\\cup B)$\n",
    "    \n",
    "    $$\\implies P(9.7 \\leq \\overline{X}_N\\leq 10.3) = F(\\overline{X}_N\\leq 10.3) - F(\\overline{X}_N\\leq 9.7) = \\phi(\\sqrt{N}\\frac{10.3-\\mu}{\\sigma}) - \\phi(\\sqrt{N}\\frac{9.7-\\mu}{\\sigma}) = \\phi(\\sqrt{N}0.3) - \\phi(-\\sqrt{N}0.3) = \\phi(\\sqrt{N}0.3) - (1 - \\phi(\\sqrt{N}0.3)) = 2\\phi(\\sqrt{N}0.3) - 1$$\n",
    "    \n",
    "    According to this result we can set the different n values and get the probablilty:\n",
    "    $$P(9.7 \\leq \\overline{X}_1\\leq 10.3) = 2\\phi(0.3) - 1 = 0.2358$$\n",
    "    $$P(9.7 \\leq \\overline{X}_{10}\\leq 10.3) = 2\\phi(0.3\\sqrt{10}) - 1 = 0.6572$$\n",
    "    $$P(9.7 \\leq \\overline{X}_{20}\\leq 10.3) = 2\\phi(0.3\\sqrt{20}) - 1 = 0.8202$$\n",
    "    \n",
    "4. In our case, $\\epsilon = 0.3$. putting it in the formula:\n",
    "    $$P(\\mu - \\epsilon \\leq \\overline{X}_N\\leq \\mu + \\epsilon) \\geq 1 - 2\\exp(-\\frac{N\\epsilon^2}{2\\sigma^2}) \\implies $$\n",
    "    \n",
    "    $$P(9.7 \\leq \\overline{X}_N\\leq 10.3) \\geq 1 - 2\\exp(-\\frac{0.09N}{2}) \\implies $$\n",
    "    \n",
    "    $$P(9.7 \\leq \\overline{X}_1\\leq 10.3) \\geq 1 - 2\\exp(-\\frac{0.09}{2}) = \\max(0,-0.9119) = 0$$\n",
    "    $$P(9.7 \\leq \\overline{X}_{10}\\leq 10.3) \\geq 1 - 2\\exp(-\\frac{0.9}{2}) = \\max(0,-0.2752) = 0$$\n",
    "    $$P(9.7 \\leq \\overline{X}_{20}\\leq 10.3) \\geq 1 - 2\\exp(-\\frac{1.8}{2}) = \\max(0,0.1868) = 0.1868$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Parametric and Non-Parametric Estimation\n",
    "---\n",
    "1. Suppose $\\hat{\\theta}$ is an estimator for an unknown parameter $\\theta$. Show that $$MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + Bias^2(\\hat{\\theta})$$\n",
    "2. Let $X_1, ..., X_N \\sim Bernoulli(p)$ and let $Y_1, ..., Y_N \\sim Bernoulli(q)$ be i.i.d. RVs. \n",
    "    * Find a (non-parametric) point estimator and the estimated standard error for $p$ (use Hoeffding).\n",
    "        * Recall the standard deviation: $$ s_N(\\hat{\\theta}) = \\sqrt{Var(\\hat{\\theta}}) $$\n",
    "    * Find an approximated 90%, 95%, 99% confidence intervals for $p$. \n",
    "    * Find the point estimator and the estimated standard error for $p-q$.\n",
    "    * Find an approximated 90% confidence interval for $p-q$.\n",
    "3. Let $X_1, ..., X_N \\sim Binomial(10, \\theta)$ be i.i.d. RVs. Estimate $\\theta$ using MLE.\n",
    "    * $P(X_i|\\theta) = \\begin{pmatrix}10 \\\\ X_i \\end{pmatrix} \\theta^{X_i}(1-\\theta)^{10-X_i}$\n",
    "4. Let $X_1, ..., X_N \\sim F$ be i.i.d. RVs, where $F$ is an arbitrary, unknown CDF. Let $\\hat{F}$ be the empirical distribution function. For a fixed $F$, use the central limit theorem (CLT) to find the limiting distribution of $\\hat{F}_n(x)$.\n",
    "5. In the lecture and tutorial, we stated the **DKW** theorem and derived a C.I. for the empirical CDF, for 1D type of data. Derive a C.I. for the empirical CDF in a general dimension, i.e., as a function of $C(k)$[see non-parametric chapter in the lectures]. If $C(k) \\sim \\exp(k)$, what does it mean about the *hardness* of the problem in high dimension?\n",
    "6. Calculate $\\mathbb{E}[\\hat{F}_N]$ and $Var[\\hat{F}_N]$ using the definition of the empirical distribution function (remember that $X_1,..., X_N$ are i.i.d. from $F$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Answer 2\n",
    "1. We add and subtract the $\\mathbb{E}[\\hat{\\theta}]$:\n",
    "\n",
    "   $$ MSE(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] + \\mathbb{E}[\\hat{\\theta}] - \\theta)^2] =$$\n",
    "   $$ = \\mathbb{E}[(\\space(\\space\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]\\space) + (\\space\\mathbb{E}[\\hat{\\theta}- \\theta]\\space)\\space)^2] = $$\n",
    "   $$ = Var(\\hat{\\theta}) + 2\\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])(\\mathbb{E}[\\hat{\\theta}- \\theta])] + Bias^2(\\hat{\\theta}) = $$\n",
    "   $$ = Var(\\hat{\\theta}) + 2\\times Bias\\times \\mathbb{E}[\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]] + Bias^2(\\hat{\\theta}) =$$\n",
    "   $$ = Var(\\hat{\\theta}) + 0 + Bias^2(\\hat{\\theta}) = $$\n",
    "   $$ = Var(\\hat{\\theta}) + Bias^2(\\hat{\\theta})$$                                            \n",
    "\n",
    "   The middle term gets zeroed as $\\mathbb{E}[\\hat{\\theta} - \\theta] = Bias(\\hat{\\theta})$ which is some constant (and we can      take it out from the expectancy operator). Therefore, the remaining term is zero as $\\mathbb{E}[\\hat{\\theta} - \\mathbb{E}      [\\hat{\\theta}]] = \\mathbb{E}[\\hat{\\theta}] - \\mathbb{E}[\\hat{\\theta}] = 0 \\hspace{15cm} \\blacksquare$\n",
    "<br>\n",
    "2. \n",
    "   * The point estimation based on none parameteric method is actually based on some functional of the $F$ distribution function.\n",
    "   <br>That is, $\\hat{p} = T(F)$:\n",
    "   <br>The $Bernoulli\\space p$ success probablity can be estimated from the distribution $F$ as $p = T(F) = 1-F(x)$ when $x \\in [0,1)$ since we accumulate only samples that were found to be 0.\n",
    "   <br>We can estimate F from the empirical distribuion $\\hat{F}_N(x) = \\frac{1}{N}\\sum_{i=1}^NI\\{X_i\\leq x\\}$\n",
    "   <br>BTW, another way is to use the fact that the expectation of a $Bernoulli$ RV is actually its probability $p$: \n",
    "   $$p = T(F) = \\int xdF(x) \\implies \\hat{p} = \\int xd\\hat{F_N}(x) = \\int x \\frac{1}{N}\\sum_1^N \\delta(x-x_i) = \\frac{1}{N}\\sum_1^N x_i = \\overline{X_N}$$\n",
    "   <br>But we will use a linear functional as follows:\n",
    "       $$p = T(F) = 1-F(x) ,x \\in [0,1) \\implies \\hat{p} = 1 - \\hat{F_n} = 1 - \\frac{1}{N}\\sum_1^N I\\{x_i \\leq x\\} = 1 - \\frac{1}{N}\\sum_1^N I\\{x_i == 0\\}$$\n",
    "   <br>We can note that it's the same as the average on such RVs.\n",
    "   <br>The standard deviation error is defined as $s_N(\\hat{p}) = \\sqrt{Var(\\hat{p})}$. \n",
    "   <br>If we put our estimator in, we get:\n",
    "   $$s_N(\\hat{p}) = \\sqrt{Var_F(\\hat{p})} = \\sqrt{\\mathbb{E}_F[(\\hat{p}-\\mathbb{E}\\hat{p})^2]} = \\sqrt{\\mathbb{E}_F[((1-\\hat{F}_N(x)) - (1-F(x)))^2]} =\\sqrt{\\mathbb{E}_F[(F(x)-\\hat{F}_N(x))^2]} = \\sqrt{\\mathbb{E}_F[(\\mathbb{E}[\\hat{F}_N(x)]-\\hat{F}_N(x))^2]} = \\sqrt{Var_F(\\hat{F}_N(x))} = \\newline = \\sqrt{\\frac{1}{N}F(x)(1-F(x))} , x \\in [0,1)$$\n",
    "   <br>The $\\hat{F}_N(x)$ is an unbiased estimation of $F$. Therefore, we replaced $\\mathbb{E}\\hat{p}$ with $(1-F(x)$.\n",
    "   <br> Also, the last equality is valid as we saw in lectures on the variance of the $\\hat{F}_N$ itself (which is actually the MSE as its estimator is unbiased). We can prove it by using the fact that $I\\{X_i\\leq x\\} \\sim Bernouli(F(x))$:\n",
    "   $$Var_F(\\hat{F}_N(x)) = \\frac{1}{N^2}\\sum_{i=1}^NVar_F(I\\{X_i\\leq x\\}) = \\frac{1}{N^2}\\sum_{i=1}^NF(x)(1-F(x))=\\frac{N}{N^2}F(x)(1-F(x))=\\frac{1}{N}F(x)(1-F(x))$$\n",
    "   <br>To conclude: $s_N(\\hat{p}) = \\sqrt{\\frac{1}{N}F(x)(1-F(x))} =\\sqrt{\\frac{1}{N}(1-p_F)p_F}$\n",
    "   * Using Hoeffding, with note that the indicator's values (as $\\hat{F_n}$ is an empirical average of indicators) are between  $b_i - a_i = 1$ . So, we get:\n",
    "   <br> $$ P(|\\hat{p}-\\mathbb{E}[\\hat{p}]|\\leq \\epsilon ) = P(|\\hat{F}_N(x) - F(x)|\\leq \\epsilon ) \\geq 1-\\alpha = 1 - 2e^{-\\frac{2N^2\\epsilon^2}{\\sum_{i=1}^N(b_i - a_i)^2}} = 1 - 2\\exp(-2N\\epsilon^2)$$\n",
    "   BTW, in this case the DKW CI is the same as Hoeffding.\n",
    "   <br>$\\implies \\epsilon = \\sqrt{\\frac{\\ln(\\frac{2}{\\alpha})}{2N}}$\n",
    "   <br>For $\\alpha = 0.1$:$$\\epsilon = \\sqrt{\\frac{\\ln(20)}{2N}} = 1.224\\sqrt{\\frac{1}{N}}$$\n",
    "   <br>For $\\alpha = 0.05$:$$\\epsilon = \\sqrt{\\frac{\\ln(40)}{2N}} = 1.358\\sqrt{\\frac{1}{N}}$$\n",
    "   <br>For $\\alpha = 0.01$:$$\\epsilon = \\sqrt{\\frac{\\ln(200)}{2N}} = 1.627\\sqrt{\\frac{1}{N}}$$\n",
    "   * The point estimator for $p-q$ is therefore:\n",
    "   $$\\hat{p} - \\hat{q} = (1 - \\hat{F_n}(x)) - (1 - \\hat{F_n}(y)) = \\hat{F_n}(y) - \\hat{F_n}(x), x,y \\in [0,1)$$\n",
    "   <br>The standard deviation error (due to i.i.d on variance and $Var(-t) = Var(t)$):\n",
    "   $$s_N(\\hat{p} - \\hat{q}) = \\sqrt{Var_F(\\hat{p}-\\hat{q})} = \\sqrt{Var_F(\\hat{p}) + Var_F(\\hat{q})} = \\sqrt{\\frac{1}{N}F(x)(1-F(x)) + \\frac{1}{N}F(y)(1-F(y))} , x,y \\in [0,1)$$\n",
    "   * The $\\mathbb{E}[\\hat(p) - \\hat(q)]$ is actually $p-q$ as the $\\hat{F_N}$ is not biased. Therefore, we can use Hoeffding to caluclate 90% CI. We also note that the range of this RV is $[-1,1]$ so $b_i-a_i=2$:\n",
    "   $$P(|(\\hat{p}-\\hat{q})-(p-q)|\\leq \\epsilon )\\geq 1-\\alpha = 1 - 2e^{-\\frac{2N^2\\epsilon^2}{\\sum_{i=1}^N(b_i - a_i)^2}} = 1 - 2\\exp(\\frac{-2N\\epsilon^2}{4})$$\n",
    "   $$\\implies \\epsilon = \\sqrt{\\frac{2\\ln(\\frac{2}{\\alpha})}{N}} = \\sqrt{\\frac{2\\ln(20)}{N}} = 2.447\\sqrt{\\frac{1}{N}}$$\n",
    "<br>\n",
    "3. We calculate $\\hat{\\theta}_{MLE} = argmax_{\\theta}L(\\theta) = argmax_{\\theta}l(\\theta)$ when $l(\\theta) = \\log(L(\\theta))$:\n",
    "    $$L(\\theta) = P(X_1,X_2\\ldots,X_N;\\theta)=\\Pi_{i=1}^NP(X_i;\\theta) \\implies$$\n",
    "    <br>$$l(\\theta) = \\sum_{i=1}^N\\log(P(X_i;\\theta)) = \\sum_{i=1}^N \\log(\\begin{pmatrix}10 \\\\ X_i \\end{pmatrix} \\theta^{X_i}(1-\\theta)^{10-X_i}) =$$\n",
    "    $$=\\sum_{i=1}^N[X_i\\log(\\theta)+(10-X_i)\\log(1-\\theta) + constant\\space vs\\space \\theta]$$\n",
    "    <br>We calculate the derivative according to $\\theta$ and compare to 0 in order to get the possible $\\theta$ that maximize the log likelihood:\n",
    "    $$\\frac{dl}{d\\theta} = \\sum_{i=1}^N[\\frac{X_i}{\\theta}-\\frac{10-X_i}{1-\\theta}] = 0 \\implies$$\n",
    "    $$\\sum_{i=1}^N[(1-\\theta)X_i - \\theta(10-X_i)] =\\sum_{i=1}^N[X_i - 10\\theta] = 0$$\n",
    "    $$\\hat{\\theta}_{MLE} = \\frac{1}{10N}\\sum_{i=1}^NX_i$$\n",
    "<br>\n",
    "4. The $\\hat{F_N}(x) = \\frac{1}{N}\\sum_{i=1}^NI\\{X_i\\leq x\\}$ is an average of N i.i.d indicator RVs that each is distirbuted $Bernoulli(F(x))$. \n",
    "<br>The mean expectation of each indicator is $P\\{X_i\\leq x\\} = F(x)$ and its variance is $F(x)(1-F(x))$. Therefore according to the LLN thorem:\n",
    "$$\\lim_{N\\rightarrow \\infty}\\mathbb{E}[\\hat{F_N}(x)] = F(x)$$ \n",
    "<br> Meaning, the expectation of the estimator converges to $F(x)$.\n",
    "<br> By the CLT law, we have that the distance between $\\hat{F_N}(x)$ and its limiting expectation $F(x)$. That is $\\hat{F_N}(x) - F(x)$ converges to a normal distributed Gaussian RV as $N$ is big enough with $\\mathcal{N}(0, \\frac{F(x)(1-F(x))}{N})$\n",
    "<br>Moreover, as $N\\rightarrow \\infty$ also the variance gets zeroed (and we have a Dirac $\\delta$ Gaussian). so $\\hat{F_N}(x)$ converges to $F(x)$ with probability 1.\n",
    "<br>\n",
    "5. As we saw in the lecture, the DKW inequality for the general case is:\n",
    "$$\\mathbb{P_F}(\\sup_x|\\hat{F_N}(x)-F(x)| \\geq \\epsilon) \\leq C(k)\\exp(-2N\\epsilon^2), \\forall \\epsilon \\implies$$\n",
    "$$\\epsilon = \\sqrt{\\frac{ln(\\frac{C(k)}{\\alpha})}{2N}}$$\n",
    "<br>if $C(k)$ is exponential in $k$, we get:\n",
    "$$\\epsilon = \\sqrt{\\frac{k + ln(\\alpha)}{2N}} = O(\\sqrt{\\frac{k}{N}})$$\n",
    "<br>Therefore, it is much harder to get a small C.I when there are several dimensions as the ratio with $\\epsilon$ (the C.I) is not $\\sqrt{ln(.)}$ but only $\\sqrt{.}$ which is almost linear, and we will need $\\sqrt{k}$ times the data samples needed to converge on one dimension to minimize the C.I.\n",
    "<br>\n",
    "6. The expectation:\n",
    "$$\\mathbb{E}_F[\\hat{F_N}(x)] = \\mathbb{E}_F[\\frac{1}{N}\\sum_{i=1}^NI\\{X_i\\leq x\\}] = \\frac{1}{N}\\sum_{i=1}^N\\mathbb{E}_F[I\\{X_i\\leq x\\}] = \\frac{1}{N}\\sum_{i=1}^NP(X_i\\leq x) = \\frac{1}{N}\\sum_{i=1}^NF(x) = F(x)$$\n",
    "<br>The variance:\n",
    "$$\\mathbb{Var}_F(\\hat{F}_N(x)) = \\frac{1}{N^2}\\sum_{i=1}^N\\mathbb{Var}_F(I\\{X_i\\leq x\\}) = \\frac{1}{N^2}\\sum_{i=1}^NF(x)(1-F(x))=\\frac{N}{N^2}F(x)(1-F(x))=\\frac{1}{N}F(x)(1-F(x))$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Exam Question - Estimators\n",
    "---\n",
    "In order to check electrical devices, a system performs repeated tests in a device until its first failure appears. The tests were performed on $N$ devices. Denote $K_i$ as the number if tests that were performed on the $i^{th}$ device (including the final test where the failure appeared), where $i \\in \\{1,...,N\\}$. Assume that $K_1,...,K_N$ are i.i.d. random variables.\n",
    "1. Find a non-parametric estimator(i.e. the plug-in/point estimator) for $\\mathbb{E}[K]$ (hint: use the Tail Sum formula).\n",
    "2. Find a non-parametric estimator for $\\hat{p}_3 \\triangleq P(K=3)$, the probability that a failure will occur in the third test.\n",
    "3. Suggest a confidence interval (CI) for $\\hat{p}_3$ for $\\alpha = 0.05, N=100$.\n",
    "\n",
    "Assume $K \\sim Geom(p)$ (Geometric Distribution), where $p$ is unknown.\n",
    "4. Calculate the MLE for $p$ and the mean $\\mu = \\mathbb{E}[K]$.\n",
    "5. Calculate the probability that the number of tests is odd, i.e., that $K$ is odd, $P(K \\text{ is odd})$. Simplify as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Answer 3  \n",
    "\n",
    "1. The $K_i \\sim Geom(p)$ when $p$ is the probability for the device to fail in our case.  \n",
    "The $\\mathbb{E}[K]$ is (according to the tail formula):  \n",
    "$$\\mu = \\mathbb{E}[K] = \\sum_{j=1}^{\\infty}\\mathbb{P}(K\\geq j) = \\sum_{j=1}^{\\infty}(1-\\mathbb{P}(K\\leq j-1)) = \\sum_{j=1}^{\\infty}(1-F(K\\leq j-1))$$\n",
    "Therefore, we can estimate it using the CDF estimator $\\hat{F_N}(j-1) = \\frac{1}{N}\\sum_{i=1}^N I\\{K_i \\leq j-1\\}$ as follows:\n",
    "$$\\hat{\\mu} = \\sum_{j=1}^{\\infty}(1-\\frac{1}{N}\\sum_{i=1}^N I\\{K_i \\leq j-1\\}) = \\sum_{j=1}^{\\infty}\\frac{1}{N}\\sum_{i=1}^N (1-I\\{K_i \\leq j-1\\}) = \\sum_{j=0}^{\\infty}\\frac{1}{N}\\sum_{i=1}^N I\\{K_i \\geq j\\}$$  \n",
    "\n",
    "2. The $\\hat{p}_3 \\triangleq P(K=3)$ is actually $P(K=3) = P(K\\geq 3) - P(K\\geq 2) = F(K\\geq 3) - F(K\\geq 2)$.  \n",
    "we plug-in the estimator and get:  \n",
    "$$\\hat{p} = \\frac{1}{N}\\sum_{i=1}^N I\\{K_i \\leq 3\\} - \\frac{1}{N}\\sum_{i=1}^N I\\{K_i \\leq 2\\} = \\frac{1}{N}\\sum_{i=1}^N I\\{K_i = 3\\} $$  \n",
    "3. The $\\hat{p}_3$ is actually a mean of indicators that are distributed $Bernoulli(q)$ when $q=P(K_i=3)$.  \n",
    "Also, the $\\hat{p}_3$ expectancy is $p_3$ (as $ \\mathbb{E}[\\hat{p}_3] = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{E}[I\\{K_i = 3\\}] = p_3$\n",
    "<br>Therefore, we can use Hoeffding to calculate the C.I. $b_i-a_i=1$ in our case:\n",
    "$$ P(|\\hat{p}_3-\\mathbb{E}[\\hat{p}_3]|\\leq \\epsilon ) = \\geq 1-\\alpha = 1 - 2e^{-\\frac{2N^2\\epsilon^2}{\\sum_{i=1}^N(b_i - a_i)^2}} = 1 - 2\\exp(-2N\\epsilon^2) \\implies$$\n",
    "$$ \\epsilon = \\sqrt{\\frac{\\ln(\\frac{2}{\\alpha})}{2N}} = \\sqrt{\\frac{\\ln(40)}{200}} = 0.1358$$  \n",
    "So we get: $p_3 \\in [\\hat{p}_3 - 0.1358, \\hat{p}_3 + 0.1358]$  \n",
    "\n",
    "4. We calculate $\\hat{p}_{MLE} = argmax_{p}L(p) = argmax_{p}l(p)$ when $l(p) = \\log(L(p))$:\n",
    "    $$L(p) = P(K_1=k_1,K_2=k_2\\ldots,K_N=k_N;p)=\\Pi_{i=1}^NP(K_i=k_i;p) \\implies$$\n",
    "    <br>$$l(p) = \\sum_{i=1}^N\\log(P(K_i=k_i;p)) = \\sum_{i=1}^N \\log((1-p)^{k_i-1}p) = \\sum_{i=1}^N[(k_i-1)\\log(1-p)+\\log(p)]$$\n",
    "    <br>We calculate the derivative according to $p$ and compare to 0 in order to get the possible $p$ that maximize the log likelihood:\n",
    "    $$\\frac{dl}{dp} = \\sum_{i=1}^N[\\frac{1-k_i}{1-p}+\\frac{1}{p}] = 0 \\implies$$\n",
    "    $$\\sum_{i=1}^N[\\frac{1-k_i}{1-p}+\\frac{1}{p}] =\\sum_{i=1}^N[\\frac{p(1-k_i)+(1-p)}{(1-p)p}] = 0 \\implies$$\n",
    "    $$\\sum_{i=1}^N[-pk_i+1] = 0 \\implies$$\n",
    "    $$\\hat{p}_{MLE} = \\frac{N}{\\sum_{i=1}^Nk_i}$$  \n",
    "As we saw, a function of $\\theta$ can be estimated using the MLE estimator just by plugging it in, as it will maximize the likelihhod of that function as well. Therefore, since the expectancy of a $Geom(p)$ RV is $\\frac{1}{p}$ we get:  \n",
    "$$\\hat{\\mu}=\\frac{1}{\\hat{p}_{MLE}}$$  \n",
    "\n",
    "5. $$P(K\\space is\\space odd) = \\sum_{i=0}^{\\infty}P(K=2i+1) = \\sum_{i=0}^{\\infty}p(1-p)^{2i} = p\\sum_{i=0}^{\\infty}((1-p)^2)^i = \\frac{p}{(1-(1-p)^2)} = \\frac{p}{(1+(1-p))(1-(1-p))} = \\frac{p}{(2-p)p} = \\frac{1}{2-p}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "## <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Question 4 - Python - Parkinson's Disease Classification Data Analysis\n",
    "---\n",
    "In this exercise, we are going to do data analysis with Python and Pandas. As this is the first \"real\" exercise, we will add guidance for some of the tasks.\n",
    "\n",
    "1. Warmup - Generate 100 samples from $\\mathcal{N}(0,1)$ (`np.random.randn`). Compute a 95% CI for the CDF. Plot the true CDF, the CDF estimation and the CI in a single plot. To estimate $\\hat{F}_n$ use a histogram (`np.histogram`). Repeat this $K=1000$ times and compute the percentage of time that the interval contained the CDF (print the value) . In addition, plot in another single figure the *true* CDF, and the best and worst experiments (use $\\max_x|F(x) - \\hat{F}_n(x)|$ as quality measure).\n",
    "    * To compare np arrays element-wise use `np.less_equal(x1, x2 + eps), np.greater_equal(x1, x2 - eps)`, use `.all()` to verify if all the comaprisons were `True`.\n",
    "    \n",
    "We are now going to perform some real data analysis on the \"Parkinson's Disease Classification Data Set\": the data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at the Department of Neurology in CerrahpaÅŸa Faculty of Medicine, Istanbul University. The control group consists of 64 healthy individuals (23 men and 41 women) with ages varying between 41 and 82. During the data collection process, the microphone is set to 44.1 KHz and following the examination, the sustained phonation of the vowel /a/ was collected from each subject with three repetitions. \n",
    "\n",
    "The features are various speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment.\n",
    "\n",
    "2. Load the data with pandas, drop the 'id' column, take a sample ($k=10$, `dataframe.sample(k)`) and view it.\n",
    "    * The filename is `pd_speech_features.csv`.\n",
    "3. Compute the empirical correlation between all pairs of features. Show the results both in a heatmap.\n",
    "    * Use pandas `.corr()` to calculate the correaltion, and `plt.imshow()` to view the heatmap (2 heatmaps, one for the correlation and one for the absolute correlation). Add a color bar using `plt.colorbar()`\n",
    "4. Print the top-20 most correlated features. Follow this steps:\n",
    "    * Take the lower triangle of the correlation matrix (as it is symmetrical and we don't care about $Corr(X_i,X_i)$). Use `np.tril()`\n",
    "    * Consider only positive correlation (because negative correlation has a different, useful meaning). You can do that by `X = X[X >0]`.\n",
    "    * From here, these are recommended steps, feel free to achieve the goal in a different way.\n",
    "        * Assignment to a pandas DataFrame: `X.loc[:,:] = np.(...)`\n",
    "        * Unstacking the DataFrame (creates a new pivot, read the doc): `df.unstack()`\n",
    "        * Sorting: `df.sort()`\n",
    "\n",
    "5. What is the meaning when 2 different features are highly correlated? From a machine learning perspective, can a classifier learn new insights from highly-correlated features? In your answer, address the process of \"feature selection\" in ML (usually performed as a pre-processing step).\n",
    "\n",
    "6. Compute the **in-class** correlation between features. Plot a heat map for each class. Address the differences between the heat maps.\n",
    "\n",
    "7. Consider the features 'numPulses' and 'app_entropy_log_5_coef'. We wish to calculate a 95% confidence interval for the correlation between these features. We will use *Bootstrapping* and the *Chebyshev inequality* (as in Tutorial 2).\n",
    "    * Implement the bootstrap algorithm to calculate the standard deviation ($\\sigma$) of the correlation.\n",
    "        * You can use the algorithm from the tutorial, but you have to modify it to support 2 arrays.\n",
    "        * The algorithm will output the empirical correlation of the two input features, and a bootstrap estimation for the std ($\\sigma$). Use `K=300` bootstrap samples and `m=100` experiments.\n",
    "        * Tips:\n",
    "            * To get values for two columns: `data[['numPulses', 'app_entropy_log_5_coef']]`\n",
    "            * To calculate correlation, check out `numpy.corrcoef`.\n",
    "    * Use $\\sigma$ to calculate a 95% CI using Chebyshev inequality.\n",
    "        * Remember to to normalize by the size of the data ($N$).\n",
    "    The final print should look something like that: `95% confidence interval for the correlation: *** ± ***`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for q-4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats  import norm\n",
    "\n",
    "#print(2* norm.cdf(0.3) - 1)\n",
    "#print(2* norm.cdf(0.3*np.sqrt(10)) - 1)\n",
    "#print(2* norm.cdf(0.3*np.sqrt(20)) - 1)\n",
    "#print(1 - 2*np.exp(-0.09/2))\n",
    "#print(1 - 2*np.exp(-0.9/2))\n",
    "#print(1 - 2*np.exp(-1.8/2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
